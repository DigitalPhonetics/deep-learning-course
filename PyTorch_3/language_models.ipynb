{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook was tested with the following Python packages:\n",
    "- torch==2.5.1\n",
    "- transformers==4.48.1\n",
    "- datasets==3.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT-style Encoder Language Models\n",
    "\n",
    "## Overview\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. This allows the model to understand the context of a word based on its surroundings, making it highly effective for various NLP tasks. Thus it can be seen as a model producing a contextualized embedding that can be used as a replacement for e.g., GloVe.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Bidirectional Context**: Unlike traditional models that read text sequentially (left-to-right or right-to-left), BERT reads the entire sequence of words at once, allowing it to understand the context of a word based on both its preceding and following words.\n",
    "\n",
    "2. **Pre-training Objectives**:\n",
    "    - **Masked Language Model (MLM)**: Randomly masks some of the tokens in the input, and the objective is to predict the original vocabulary id of the masked word based only on its context.\n",
    "    - **Next Sentence Prediction (NSP)**: Predicts whether a given pair of sentences is consecutive in the original text, helping the model understand sentence relationships.\n",
    "\n",
    "## Applications\n",
    "\n",
    "BERT can be fine-tuned for various downstream tasks, including:\n",
    "\n",
    "- **Text Classification**: Sentiment analysis, spam detection, etc.\n",
    "- **Question Answering**: Extracting answers from a given context.\n",
    "- **Named Entity Recognition (NER)**: Identifying entities like names, dates, and locations in text.\n",
    "- **Text Summarization**: Generating concise summaries of longer texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first install the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the SST2 dataset. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with pre-trained models, you will have to download the weights from somewhere.\\\n",
    "These weights have to fit the model architecture, therefore we rely on a library for the models and its weights.\\\n",
    "We here use the Hugging Face transformers library for loading the model and its tokenizer.\n",
    "\n",
    "There are so many encoder language models; here we use RoBERTa which is an optimized (in terms of hyperparameters) version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the model's architecture by printing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we prepare the dataset, but this time the tokenizer does most of the work converting text to token ids for the loaded model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate if you want to use a subset of the dataset for faster training\n",
    "USE_SUBSET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize(examples, tokenizer):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, return_attention_mask=False)\n",
    "\n",
    "\n",
    "# Prepare the datasets\n",
    "tokenized_datasets = dataset.map(tokenize, fn_kwargs=dict(tokenizer=tokenizer), batched=True, num_proc=10)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Use a subset for quick training\n",
    "if USE_SUBSET:\n",
    "    train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "    val_dataset = val_dataset.shuffle(seed=42).select(range(100))\n",
    "    test_dataset = test_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "# Print the features\n",
    "print(train_dataset.features)\n",
    "# Print the first example\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Feed the first training example through the LM.\n",
    ">Note: The model takes the token ids with the argument `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model on the first example\n",
    "output = roberta(input_ids=torch.tensor(train_dataset[0][\"input_ids\"]).unsqueeze(0))\n",
    "print(output.last_hidden_state.shape)\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's output is a contextualized representation of the input, and therefor can be used as such in your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, freeze_embedder=False):\n",
    "        super().__init__()\n",
    "        # We are using the transformer model as base_model\n",
    "        self.base_model = base_model\n",
    "        # Freeze the base_model\n",
    "        if freeze_embedder:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # We add a linear layer on top of the base_model\n",
    "        self.classifier = torch.nn.Linear(base_model.config.hidden_size, 2)\n",
    "        # Initialize the classifier weights\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight) # Xavier uniform initialization\n",
    "        torch.nn.init.zeros_(self.classifier.bias) # Initialize bias with zeros\n",
    "\n",
    "    def forward(self, **model_inputs):\n",
    "        # model_inputs is a dict\n",
    "        # Pass the inputs to the model to produce an embedding\n",
    "        base_model_output = self.base_model(**model_inputs)\n",
    "        # Pass the embedding through the classifier\n",
    "        # here we use the pooler_output as the representation of the sentence (depends on the model)\n",
    "        output = self.classifier(base_model_output.pooler_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = SentimentAnalysisModel(roberta, freeze_embedder=False).to(device) # Set to True to freeze the embedder for faster training\n",
    "model(input_ids=torch.tensor([train_dataset[0][\"input_ids\"]], device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only passed the token ids to the model.\\\n",
    "However, there are more inputs for transformer-based models which you need to be aware of:\n",
    "\n",
    "1. **input_ids**: Token IDs to be fed to the model.\n",
    "2. **attention_mask**: Mask to avoid performing attention on padding token indices.\n",
    "3. **token_type_ids**: Segment token indices to indicate different portions of the inputs (used in models like BERT for tasks like question answering).\n",
    "4. **position_ids**: Indices of positions of each input sequence token in the position embeddings.\n",
    "\n",
    "If you don't pass them into the model, the library will take care of it!\\\n",
    "However, if you perform padding during batching, the model might not know where padding was applied and therefore you will also have to provide the `attention_mask`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the processed data and the model in place, we create the appropriate dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(features):\n",
    "    # We need to pad the input to make sure all sentences have the same length\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(f[\"input_ids\"]) for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    if \"attention_mask\" in features[0]:\n",
    "        # If the features contain attention_mask, we should pad them as well\n",
    "        attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(f[\"attention_mask\"]) for f in features],\n",
    "            batch_first=True,\n",
    "            padding_value=0,\n",
    "        )\n",
    "    else:\n",
    "        # We need to create an attention mask from input_ids\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).int()\n",
    "    labels = torch.tensor([f[\"label\"] for f in features])\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing the collate function ourselbes, the transformers library provides them for different applications as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also write an evaluation function that computes the accuracy of all samples in a dataloader using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, dataloader, device=None):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluation accuracy on validation data: {evaluate(model, val_dataloader, device=device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a training loop that is just the same as before.\\\n",
    "We have to make sure to feed in the correct arguments to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 3\n",
    "metric_dict = {\"Loss\": \"-\", \"Val Acc\": evaluate(model, val_dataloader, device=device)}\n",
    "\n",
    "with tqdm(\n",
    "    total=num_epochs * len(train_dataloader), desc=\"Training\", unit=\"batch\"\n",
    ") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model in training mode\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metric_dict[\"Loss\"] = loss.item()\n",
    "            pbar.set_postfix(metric_dict)\n",
    "            pbar.update(1)\n",
    "        metric_dict[\"Val Acc\"] = evaluate(model, val_dataloader, device=device)\n",
    "        pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the transformers library makes it simple to use LMs as it includes task-specific models for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobertaForSequenceClassification model can be used for text classification tasks like sentiment analysis\n",
    "# It has a sequence classification head, that is a linear layer on top of the RoBERTa model that outputs a classification label\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use their Trainer API so that you don't have to implement the training loop again and again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the metrics\n",
    "def compute_metrics(pred_and_label: EvalPrediction):\n",
    "    return {\n",
    "        \"accuracy\": (pred_and_label.predictions.argmax(axis=-1) == pred_and_label.label_ids)\n",
    "        .mean()\n",
    "        .item()\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    optim=\"adamw_hf\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    use_cpu=False,\n",
    "    eval_on_start=True,\n",
    "    save_strategy=\"no\",  # We will not save the model for now to save disk space\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,  # enables padding of batches\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation Language Models with Prompting\n",
    "\n",
    "Text generation language models, such as GPT-3, can be used with prompting to perform various natural language processing tasks, including sentiment analysis.\\\n",
    "Prompting involves providing the model with a specific input or \"prompt\" that guides it to generate the desired output.\\\n",
    "This technique leverages the model's pre-trained knowledge to perform tasks without requiring additional fine-tuning.\n",
    "\n",
    "#### Applying Prompting to Sentiment Analysis\n",
    "\n",
    "To use a text generation model for sentiment analysis, you can craft a prompt that instructs the model to classify the sentiment of a given text. The prompt should be designed to elicit a response that indicates whether the sentiment is positive or negative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformers library makes it simple to load a text generation model.\n",
    ">Note that they can be quite large and potentially do not fit or run slowly on your CPU/RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    trust_remote_code=True,  # Trust the remote code; this is required for some models, but always check the code first!\n",
    "    device=device,  # Set this to \"cuda\" for GPU acceleration if available\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for less memory usage and faster inference\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format for Chat-Based Decoder Models\n",
    "\n",
    "Chat-based decoder models, such as GPT-3, typically require inputs in a specific format to generate coherent and contextually relevant responses.\\\n",
    "The input format generally consists of a sequence of messages, each with a role and content.\\\n",
    "The roles depend on the model, and often are \"system\", \"user\", or \"assistant\".\n",
    "\n",
    "#### Example Input Format\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your task is to perform sentiment analysis. Classify the sentiment of the provided text into 'negative' or 'positive' and return only this label.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"This movie was the worst movie I have ever seen.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "1. **Role**: Indicates the role of the message sender. Common roles include:\n",
    "   - `system`: Provides instructions or context for the conversation.\n",
    "   - `user`: Represents the input from the user.\n",
    "   - `assistant`: Represents the response from the model.\n",
    "\n",
    "2. **Content**: The actual text of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your task is to perform sentiment analysis. Classify the sentiment of the provided text into 'negative' or 'positive' and return only this label.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"This movie was the worst movie I have ever seen.\"},\n",
    "]\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 3,  # maximum number of tokens to generate\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,  # temperature for sampling (on if do_sample=True)\n",
    "    \"do_sample\": False,  # whether to sample from the output distribution\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more that you can do with text generation models!\\\n",
    "For example, in-context learning (sometimes also called demonstration learning) is a technique where the model is provided with examples of the task it needs to perform within the prompt itself.\\\n",
    "This helps the model understand the task better and generate more accurate responses.\\\n",
    "For sentiment analysis, you can provide a few examples of sentences along with their sentiment labels in the prompt.\\\n",
    "The model will then use these examples to infer the sentiment of new sentences.\n",
    "\n",
    "Also, constraining the output tokens can help guiding the model to generate expected outputs and make it easier to parse the output."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
