{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas:\n",
    "\n",
    "\n",
    "1. **Transformers and BERT**:\n",
    "    - **Overview of the Transformer architecture**:\n",
    "        - Explanation of self-attention mechanism and multi-head attention.\n",
    "        - Understanding positional encoding and its importance.\n",
    "        - Comparison between RNNs and Transformers.\n",
    "    - **Implementing a Transformer model in PyTorch**:\n",
    "        - Building the Transformer encoder and decoder from scratch.\n",
    "        - Training the Transformer model on a language modeling task (e.g., text generation).\n",
    "    - **Introduction to BERT (Bidirectional Encoder Representations from Transformers)**:\n",
    "        - Understanding the pre-training objectives: Masked Language Model (MLM) and Next Sentence Prediction (NSP).\n",
    "        - Fine-tuning BERT for a specific task using PyTorch (e.g., sentiment analysis, question answering).\n",
    "        - Exploring variations of BERT like RoBERTa, DistilBERT, and their use cases.\n",
    "\n",
    "2. **Language Model Evaluation**:\n",
    "    - **Metrics for evaluating language models**:\n",
    "        - Explanation of perplexity and its calculation.\n",
    "        - Understanding BLEU score for evaluating translation tasks.\n",
    "        - Introduction to other metrics like ROUGE, METEOR, and their applications.\n",
    "    - **Implementing evaluation metrics in PyTorch**:\n",
    "        - Writing functions to calculate perplexity for a given language model.\n",
    "        - Implementing BLEU score calculation for evaluating Seq2Seq models.\n",
    "        - Using libraries like `nltk` or `sacrebleu` for metric calculations.\n",
    "    - **Practical evaluation**:\n",
    "        - Evaluating the performance of different models (RNN, LSTM, GRU, Transformer) on the same dataset.\n",
    "        - Analyzing the results and discussing the strengths and weaknesses of each model.\n",
    "        - Visualizing evaluation metrics using tools like Matplotlib or Seaborn.\n",
    "\n",
    "3. **Advanced Topics**:\n",
    "    - Exploring recent advancements like GPT-3 and their implementations.\n",
    "    - Ethical considerations and biases in language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model with tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, return_attention_mask=False)\n",
    "\n",
    "\n",
    "# Prepare the datasets\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True, num_proc=10)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Use a subset for quick training\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(100))\n",
    "val_dataset = val_dataset.shuffle(seed=42).select(range(100))\n",
    "\n",
    "print(train_dataset.features)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model on the first example\n",
    "output = roberta(input_ids=torch.tensor([train_dataset[0][\"input_ids\"]]))\n",
    "print(output.last_hidden_state.shape)\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's output is a contextualized representation of the input, and therefor can be used as such in your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(torch.nn.Module):\n",
    "    def __init__(self, embedder, freeze_embedder=True):\n",
    "        super().__init__()\n",
    "        # We are using the transformer model as embedder\n",
    "        self.embedder = embedder\n",
    "        # Freeze the embedder\n",
    "        if freeze_embedder:\n",
    "            for param in self.embedder.parameters():\n",
    "                param.requires_grad = False\n",
    "        # We add a linear layer on top of the embedder\n",
    "        self.classifier = torch.nn.Linear(embedder.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, **model_inputs):\n",
    "        # Pass the inputs to the model to produce an embedding\n",
    "        embeddings = self.embedder(**model_inputs)\n",
    "        # Pass the embedding through the classifier\n",
    "        # here we use the pooler_output as the representation of the sentence (depends on the model)\n",
    "        output = self.classifier(embeddings.pooler_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = SentimentAnalysisModel(roberta)\n",
    "model(input_ids=torch.tensor([train_dataset[0][\"input_ids\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Describe model inputs, like input_ids, attention_mask and token_type_ids (latter is not used by Roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader for batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(features):\n",
    "    # We need to pad the input to make sure all sentences have the same length\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.tensor(f[\"input_ids\"]) for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels = torch.tensor([f[\"label\"] for f in features])\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "    if \"attention_mask\" in features[0]:\n",
    "        batch[\"attention_mask\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(f[\"attention_mask\"]) for f in features],\n",
    "            batch_first=True,\n",
    "            padding_value=0,\n",
    "        )\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset.with_format(columns=[\"input_ids\", \"label\"]),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(model, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "print(f\"Evaluation accuracy on validation data: {evaluate(model, val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "metric_dict = {\"Loss\": \"-\", \"Val Acc\": evaluate(model, val_dataloader)}\n",
    "\n",
    "with tqdm(\n",
    "    total=num_epochs * len(train_dataloader), desc=\"Training\", unit=\"batch\"\n",
    ") as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model in training mode\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"labels\"]\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metric_dict[\"Loss\"] = loss.item()\n",
    "            pbar.set_postfix(metric_dict)\n",
    "            pbar.update(1)\n",
    "        metric_dict[\"Val Acc\"] = evaluate(model, val_dataloader)\n",
    "        pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the transformers library makes it simple to use LMs as it includes task-specific models for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobertaForSequenceClassification model can be used for text classification tasks like sentiment analysis\n",
    "# It has a sequence classification head, that is a linear layer on top of the RoBERTa model that outputs a classification label\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    optim=\"sgd\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    use_cpu=True,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=tokenizer, # enables padding of batches\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
