{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch POS Tagging\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- huggingface datasets\n",
    "- tqdm\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/users0/tillipl/.local/lib/python3.12/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from spacy) (2.10.5)\n",
      "Requirement already satisfied: jinja2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/users0/tillipl/.local/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "# %conda install spacy # or install using conda\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<frozen runpy>:222\u001b[0m, in \u001b[0;36mrun_module\u001b[0;34m(mod_name, init_globals, run_name, alter_sys)\u001b[0m\n",
      "File \u001b[0;32m<frozen runpy>:148\u001b[0m, in \u001b[0;36m_get_module_details\u001b[0;34m(mod_name, error)\u001b[0m\n",
      "File \u001b[0;32m<frozen runpy>:112\u001b[0m, in \u001b[0;36m_get_module_details\u001b[0;34m(mod_name, error)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/spacy/compat.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[1;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/thinc/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     CupyOps,\n\u001b[1;32m      3\u001b[0m     MPSOps,\n\u001b[1;32m      4\u001b[0m     NumpyOps,\n\u001b[1;32m      5\u001b[0m     Ops,\n\u001b[1;32m      6\u001b[0m     get_current_ops,\n\u001b[1;32m      7\u001b[0m     get_ops,\n\u001b[1;32m      8\u001b[0m     set_current_ops,\n\u001b[1;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[1;32m     10\u001b[0m     use_ops,\n\u001b[1;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[1;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/thinc/backends/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/thinc/backends/cupy_ops.py:16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     is_cupy_array,\n\u001b[1;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     torch2xp,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/thinc/backends/numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users0/tillipl/.conda/envs/vqa-cl/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3040: UserWarning: Unknown failure executing module: <spacy>\n",
      "  warn('Unknown failure executing module: <%s>' % mod_name)\n"
     ]
    }
   ],
   "source": [
    "# download resources for english\n",
    "# `run` has to be replaced by `python` if run in a shell\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the POS tagging dataset from the Hugging Face hub and prepares it for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"batterydata/pos_tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the loaded dataset followed by its training and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 13054\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 1451\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['words', 'labels'],\n",
      "    num_rows: 13054\n",
      "})\n",
      "Dataset({\n",
      "    features: ['words', 'labels'],\n",
      "    num_rows: 1451\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"])\n",
    "print(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_CACHE = os.path.expanduser(\"./glove/\")\n",
    "DATASET_ROOT = os.path.expanduser(\"./\")\n",
    "BATCH_SIZE = 16 # make sure that batches fit into your device's memory but note that the batch size influences your training (it is a hyperparameter)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 'cuda' for GPU (optional specify device id) and 'cpu' for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our neural network consists of one fully connected linear layer\n",
    "\n",
    "The softmax is part of the loss function in PyTorch, so you can omit this in the forward function.\n",
    "\n",
    "The embedding layer\n",
    "- maps from indices to vectors\n",
    "- is not trained (freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    # this resembles a really simple neural network: an embedding layer followed by a fully\n",
    "    # connected linear layer such that predictions are computed for each token in the sequence\n",
    "    # and batch independently\n",
    "    def __init__(self, embedding_vectors, num_classes, hidden_dim):\n",
    "        super().__init__()\n",
    "        # PyTorch's embedding layer maps from indices to embeddings, freeze will tell PyTorch to\n",
    "        # not train this layer, i.e. not modifying any weight\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_vectors, freeze=True)\n",
    "        # a fully connected linear layer mapping the embedded vector to a vector of fixed size\n",
    "        # (num_classes in this case)\n",
    "        self.hidden_layer = torch.nn.Linear(embedding_vectors.size(1), hidden_dim)\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # simple forwarding through our model\n",
    "        # PyTorch takes care of keeping track of the operations for the backward pass\n",
    "        emmedded_inputs = self.embedding(inputs)\n",
    "        z_1 = self.hidden_layer(emmedded_inputs)\n",
    "        a_1 = torch.nn.functional.leaky_relu(z_1, negative_slope=0.2)\n",
    "        z_2 = self.output_layer(a_1)        \n",
    "        return z_2 # softmax is applied in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GloVe\n",
    "GloVe embeddings were trained with a special objective.\n",
    "Word pairs share the same underlying concept: Vector differences should be roughly equal.\n",
    "\n",
    "<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\" width=500/>\\\n",
    "source: https://nlp.stanford.edu/projects/glove/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create iterator such that each iteration returns a batch from shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt', 'glove.6B.50d.txt']\n"
     ]
    }
   ],
   "source": [
    "# Download the GloVe embeddings\n",
    "glove = hf_hub_download(\"stanfordnlp/glove\", \"glove.6B.zip\")\n",
    "\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    print(f.namelist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the 'glove.6B.300d.txt' file from the downloaded GloVe archive and print the first few lines for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'the 0.04656 0.21318 -0.0074364 -0.45854 -0.035639 0.23643 -0.28836 0.21521 -0.13486 -1.6413 -0.26091 0.032434 0.056621 -0.043296 -0.021672 0.22476 -0.075129 -0.067018 -0.14247 0.038825 -0.18951 0.29977 0.39305 0.17887 -0.17343 -0.21178 0.23617 -0.063681 -0.42318 -0.11661 0.093754 0.17296 -0.33073 0.49112 -0.68995 -0.092462 0.24742 -0.17991 0.097908 0.083118 0.15299 -0.27276 -0.038934 0.54453 0.53737 0.29105 -0.0073514 0.04788 -0.4076 -0.026759 0.17919 0.010977 -0.10963 -0.26395 0.07399 0.26236 -0.1508 0.34623 0.25758 0.11971 -0.037135 -0.071593 0.43898 -0.040764 0.016425 -0.4464 0.17197 0.046246 0.058639 0.041499 0.53948 0.52495 0.11361 -0.048315 -0.36385 0.18704 0.092761 -0.11129 -0.42085 0.13992 -0.39338 -0.067945 0.12188 0.16707 0.075169 -0.015529 -0.19499 0.19638 0.053194 0.2517 -0.34845 -0.10638 -0.34692 -0.19024 -0.2004 0.12154 -0.29208 0.023353 -0.11618 -0.35768 0.062304 0.35884 0.02906 0.0073005 0.0049482 -0.15048 -0.12313 0.19337 0.12173 0.44503 0.25147 0.10781 -0.17716 0.038691 0.08153 0.14667 0.063666 0.061332 -0.075569 -0.37724 0.01585 -0.30342 0.28374 -0.042013 -0.040715 -0.15269 0.07498 0.15577 0.10433 0.31393 0.19309 0.19429 0.15185 -0.10192 -0.018785 0.20791 0.13366 0.19038 -0.25558 0.304 -0.01896 0.20147 -0.4211 -0.0075156 -0.27977 -0.19314 0.046204 0.19971 -0.30207 0.25735 0.68107 -0.19409 0.23984 0.22493 0.65224 -0.13561 -0.17383 -0.048209 -0.1186 0.0021588 -0.019525 0.11948 0.19346 -0.4082 -0.082966 0.16626 -0.10601 0.35861 0.16922 0.07259 -0.24803 -0.10024 -0.52491 -0.17745 -0.36647 0.2618 -0.012077 0.08319 -0.21528 0.41045 0.29136 0.30869 0.078864 0.32207 -0.041023 -0.1097 -0.092041 -0.12339 -0.16416 0.35382 -0.082774 0.33171 -0.24738 -0.048928 0.15746 0.18988 -0.026642 0.063315 -0.010673 0.34089 1.4106 0.13417 0.28191 -0.2594 0.055267 -0.052425 -0.25789 0.019127 -0.022084 0.32113 0.068818 0.51207 0.16478 -0.20194 0.29232 0.098575 0.013145 -0.10652 0.1351 -0.045332 0.20697 -0.48425 -0.44706 0.0033305 0.0029264 -0.10975 -0.23325 0.22442 -0.10503 0.12339 0.10978 0.048994 -0.25157 0.40319 0.35318 0.18651 -0.023622 -0.12734 0.11475 0.27359 -0.21866 0.015794 0.81754 -0.023792 -0.85469 -0.16203 0.18076 0.028014 -0.1434 0.0013139 -0.091735 -0.089704 0.11105 -0.16703 0.068377 -0.087388 -0.039789 0.014184 0.21187 0.28579 -0.28797 -0.058996 -0.032436 -0.0047009 -0.17052 -0.034741 -0.11489 0.075093 0.099526 0.048183 -0.073775 -0.41817 0.0041268 0.44414 -0.16062 0.14294 -2.2628 -0.027347 0.81311 0.77417 -0.25639 -0.11576 -0.11982 -0.21363 0.028429 0.27261 0.031026 0.096782 0.0067769 0.14082 -0.013064 -0.29686 -0.079913 0.195 0.031549 0.28506 -0.087461 0.0090611 -0.20989 0.053913\\n'\n",
      "b', -0.25539 -0.25723 0.13169 -0.042688 0.21817 -0.022702 -0.17854 0.10756 0.058936 -1.3854 0.58509 0.036501 -0.19846 0.19613 0.40929 0.15702 -0.15305 0.050447 0.30045 -0.11295 -0.017043 0.18593 0.19982 0.20053 -0.63141 -0.12622 0.2951 -0.26282 -0.15831 0.0012383 0.011784 0.58758 -0.15914 0.27731 -0.82343 -0.21134 0.013414 0.19637 -0.4147 0.0010276 0.13422 -0.14205 0.051545 0.34993 -0.29868 -0.3209 0.19566 0.47886 0.10744 0.010004 0.18503 0.080694 0.20739 -0.097365 -0.039448 0.020151 -0.17378 0.25679 0.24198 -0.351 0.18759 0.0063857 0.18395 -0.13929 0.0081855 -0.63109 0.29832 0.31731 0.13022 -0.32284 -0.050343 -0.114 0.12097 0.14687 -0.33244 -0.055789 -0.05849 0.27551 -0.043855 0.039664 0.15162 -0.086627 0.067729 0.23146 0.015351 -0.15142 -0.031975 0.45181 -0.068806 -0.077058 0.055193 0.054596 -0.24708 0.031113 -0.12826 0.12782 -0.46708 -0.026264 0.010387 -0.33174 0.17277 -0.26894 0.20467 -0.16181 -0.041519 -0.014878 0.10279 0.18868 -0.23396 -0.018436 -0.14747 -0.32685 -0.022055 -0.054 0.16264 0.27095 -0.22792 -0.0077006 0.11206 -0.039787 -0.11906 0.021773 0.05528 -0.13318 -0.056867 0.008304 -0.027021 0.23447 0.086864 0.12009 -0.30726 0.0024735 0.29041 -0.044887 0.12297 0.13077 0.090807 -0.39141 0.080546 0.18724 -0.097481 0.10397 0.11492 0.17775 -0.18167 0.24652 0.20136 -0.23395 -0.35018 -0.14061 0.17091 -0.095465 -0.10962 -0.09836 0.15344 0.08868 -0.22048 -0.13803 -0.11288 -0.08534 0.072735 -0.12732 -0.1964 -0.10586 0.0020616 0.13496 0.058912 -0.043979 -0.091375 0.24408 0.16872 0.24297 -0.43983 0.47089 -0.018595 0.16146 0.19828 -0.17237 -0.0026998 0.52097 -0.080197 0.43324 -0.066261 0.04324 0.084954 -0.14836 -0.41936 0.15988 -0.18411 0.1321 0.27476 0.27279 -0.13465 -0.091238 -0.32523 0.27936 0.023296 -0.33472 0.016878 -0.055544 0.92915 -0.33914 -0.14791 0.017301 0.18272 0.35108 -0.11438 0.13228 -0.021064 -0.27453 -0.10081 -0.046296 0.21689 -0.056319 0.14651 -0.023536 0.068026 -0.045453 -0.23851 -0.33868 0.31396 -0.031914 -0.019217 0.0018715 -0.13328 0.070148 -0.039761 0.070801 0.0018422 -0.12646 0.028675 -0.095728 0.26673 -0.35536 0.15286 0.064565 0.12647 0.23397 -0.046058 0.13519 -0.14549 0.23031 0.42066 0.16267 -0.16541 -0.0020155 0.080653 -0.30025 -0.076014 0.070612 0.3157 0.05352 -0.10721 -0.1366 0.32214 0.2004 0.11609 -0.22501 0.12155 -0.10851 -0.063187 -0.24553 -0.059751 0.068787 -0.11627 -0.0083402 0.0052044 -0.20159 -0.023663 0.17562 -0.31475 -0.11162 -0.12492 0.10949 -0.26913 0.34893 -1.6997 -0.2447 0.30292 0.05672 -0.31737 0.083612 0.095949 -0.1759 0.10235 0.36808 -0.3438 0.20607 0.19135 0.10992 0.075968 -0.014359 -0.073794 0.22176 0.14652 0.56686 0.053307 -0.2329 -0.12226 0.35499\\n'\n",
      "b'. -0.12559 0.01363 0.10306 -0.10123 0.098128 0.13627 -0.10721 0.23697 0.3287 -1.6785 0.22393 0.12409 -0.086708 0.3301 0.34375 -0.00087582 -0.29658 0.24417 -0.11592 -0.035742 -0.01083 0.20776 0.29285 -0.073491 -0.18598 -0.2009 -0.095366 0.0063732 -0.1362 0.092028 -0.039957 0.19027 -0.10456 0.002767 -0.71742 -0.12915 -0.0013451 0.27002 -0.053023 0.22148 0.13881 -0.15051 -0.1915 0.16402 0.097484 0.056841 0.39789 0.40725 0.14802 0.21569 -0.10671 -0.10232 0.02481 -0.221 -0.01072 0.14234 -0.28242 0.19254 0.08672 -0.3897 0.11321 0.0013779 0.0064009 -0.16206 -0.082153 -0.55397 0.36789 -0.0040159 0.2071 -0.37157 0.25135 -0.19544 -0.047059 0.17155 -0.24036 -0.046086 0.19429 -0.18939 -0.0071974 0.069481 0.059175 -0.17585 0.10653 0.16933 -0.036122 0.029911 -0.1183 0.13916 -0.037951 0.1069 -0.26069 -0.10307 -0.12272 -0.15032 -0.042409 0.013354 -0.2851 0.011248 0.16073 -0.16384 0.21233 -0.18476 -0.00090874 0.066687 0.16918 -0.35004 0.099016 0.46393 -0.19462 0.10346 -0.25668 -0.36516 -0.18963 -0.21933 0.024634 0.065627 -0.1112 -0.164 0.010874 -0.084688 -0.14923 -0.070223 0.028887 0.083497 -0.016193 -0.0024926 0.17186 0.0098749 0.080237 0.14774 0.043206 0.27716 0.57697 -0.041297 0.12765 -0.091517 0.14132 0.087579 0.093224 0.015346 -0.19856 0.017277 -0.10708 -0.013059 -0.37227 0.078568 0.16677 -0.15359 -0.33294 0.036986 0.11697 0.039781 0.038464 -0.16247 0.4128 -0.077491 0.04549 0.1133 0.0082177 -0.25052 0.070966 -0.11388 -0.11503 -0.11014 0.10499 0.15878 -0.27023 -0.011006 0.00076057 0.33902 0.25564 0.16342 -0.56019 0.13055 0.076311 -0.028334 0.28721 -0.027844 -0.11561 0.34925 -0.1242 0.21405 0.24116 -0.031343 0.10913 -0.24755 -0.045429 -0.082178 -0.18831 0.18446 -0.097074 0.32395 0.10658 -0.26676 -0.27311 0.017181 0.25796 -0.28048 0.3079 -0.218 0.87415 -0.12297 0.10991 -0.29797 0.13394 0.10615 -0.10789 -0.35976 -0.18311 -0.45133 0.034967 -0.19847 0.21965 0.08152 0.2581 0.040173 0.031394 0.19069 0.0758 -0.060638 0.20739 0.009839 -0.2693 0.066515 -0.10711 0.0059916 0.23284 -0.058663 0.098993 -0.081464 0.067004 -0.14305 0.25506 -0.31971 -0.03107 -0.092451 0.2944 0.28947 -0.059804 0.24286 -0.16755 0.042031 0.51261 0.24525 -0.65983 0.062456 0.052204 -0.025717 -0.080613 0.080869 0.22821 -0.10217 -0.20719 -0.012123 0.34916 0.086527 0.066288 -0.099828 0.25843 0.11943 -0.13667 -0.43962 0.23704 0.031296 0.074701 -0.22387 0.0078162 -0.19016 0.044444 0.20191 -0.20814 -0.28382 0.10427 -0.21098 0.18865 0.31659 -2.0753 -0.071045 0.52419 0.056023 -0.25295 -0.062168 -0.10989 -0.35755 -0.079244 0.37472 -0.28353 0.16337 0.11165 -0.098002 0.060148 -0.15619 -0.11949 0.23445 0.081367 0.24618 -0.15242 -0.34224 -0.022394 0.13684\\n'\n",
      "b'of -0.076947 -0.021211 0.21271 -0.72232 -0.13988 -0.12234 -0.17521 0.12137 -0.070866 -1.5721 -0.22464 0.04269 -0.4018 0.21006 0.014288 0.41628 0.017165 0.071732 0.0069246 0.18107 -0.15412 0.14933 -0.030493 0.29918 0.029479 -0.036147 -0.061125 0.083918 -0.12398 -0.10077 -0.0054142 0.3371 -0.25612 0.44388 -0.68922 0.1802 0.34898 -0.052284 -0.26226 -0.47109 0.21647 -0.4002 -0.049986 0.011376 0.54994 -0.22791 0.095873 0.47693 -0.056727 -0.17895 0.11756 0.14662 0.048948 0.13587 -0.093821 0.45968 -0.32062 0.29911 0.20656 -0.18503 -0.2769 -0.022545 0.70698 -0.23815 0.16437 -0.55044 -0.0010615 0.12266 0.11898 0.23985 0.29815 0.013207 0.16316 -0.61334 -0.37051 0.19444 -0.13621 -0.30426 -0.37715 0.065299 -0.15995 -0.56516 0.074696 0.40184 0.19328 0.041802 0.20572 0.28971 0.34783 0.33873 -0.10052 -0.16397 -0.15236 -0.086815 0.36522 0.14969 -0.40859 0.23106 0.17162 -0.60545 0.086019 0.37043 0.17937 -0.40282 -0.62471 -0.055919 0.15092 0.12554 -0.45344 0.34417 0.40042 -0.049512 -0.29969 -0.31761 0.30023 0.090029 0.3106 -0.033077 -0.21995 -0.40396 -0.34443 -0.21248 -0.37636 0.21835 -0.1785 -0.17261 0.16391 0.22753 0.2686 0.57541 -0.14912 0.20413 0.22187 -0.27014 0.068253 0.29115 -0.067943 0.10623 -0.16281 0.19939 -0.48613 0.035688 -0.12373 0.13707 0.33359 -0.12713 -0.31711 -0.13962 -0.04288 -0.0014614 0.76883 -0.41705 -0.092911 0.16315 0.29202 0.12119 -0.076683 0.14131 -0.093406 -0.042796 0.13738 0.014278 0.11918 -0.34215 -0.19076 -0.12499 0.24648 0.42259 0.091966 0.45351 0.14437 0.1878 -0.85876 0.059621 -0.32242 0.28627 0.12427 0.0090984 -0.1891 0.16638 0.099881 -0.048553 -0.026257 0.099904 0.12406 -0.015416 -0.29707 -0.4044 -0.17258 0.36468 -0.014118 -0.11889 -0.11686 -0.14124 0.28012 0.067644 0.1485 -0.35702 0.29626 0.36004 1.019 -0.067307 -0.11588 -0.2178 0.070191 0.23154 -0.13849 0.26441 0.28742 0.1941 -0.0060504 0.44105 0.12416 -0.27745 -0.25729 0.10992 0.18362 -0.34522 -0.21861 -0.18825 -0.037454 -0.20862 -0.25216 0.060842 0.068595 0.10275 0.10745 -0.061288 0.19725 -0.27739 -0.022559 0.052794 -0.24083 0.09199 0.30959 0.054999 0.063676 -0.087357 -0.34495 0.22793 -0.42405 0.24536 0.55708 0.19126 -0.797 -0.2048 0.32545 0.09235 0.084791 -0.16433 -0.066568 -0.099249 0.31526 -0.44465 0.087281 0.3288 -0.017809 -0.23855 -0.12848 0.041509 0.46728 0.48214 0.10548 0.065805 0.067221 0.13321 -0.27856 0.015532 0.30026 0.38748 -0.14401 -0.16131 0.17678 0.16448 -0.3244 0.007937 -2.2836 0.096945 0.66131 0.16857 -0.028877 -0.10791 -0.027445 -0.25695 0.046686 0.23087 -0.076458 0.27127 0.25185 0.054947 -0.36673 -0.38603 0.3029 0.015747 0.34036 0.47841 0.068617 0.18351 -0.29183 -0.046533\\n'\n",
      "b'to -0.25756 -0.057132 -0.6719 -0.38082 -0.36421 -0.082155 -0.010955 -0.082047 0.46056 -1.8477 -0.11258 -0.12955 0.27254 0.0072891 0.26038 0.12096 -0.23193 0.03226 -0.29472 -0.67594 -0.33844 -0.23297 0.1102 0.18816 -0.45184 -0.33833 0.11274 0.4949 -0.042132 0.079961 -0.013146 0.062284 0.20223 0.038279 -1.1154 -0.1214 0.089846 0.29702 -0.055794 -0.46021 -0.13194 0.087357 -0.27865 0.14981 0.25536 0.16698 -0.04452 0.067588 -0.11772 -0.13452 0.28694 -0.39844 -0.12806 -0.47818 0.067802 0.20353 -0.30677 0.60789 -0.18588 0.11997 -0.040508 -0.06586 0.30621 -0.055824 0.039448 -0.4557 0.21081 0.25889 0.14666 0.3095 0.14343 0.10524 0.15788 0.103 0.32211 -0.27939 -0.17139 0.32202 0.10784 -0.28209 0.12611 -0.23913 -0.089638 -0.39179 -0.26402 0.36796 -0.23691 0.62503 -0.027226 -0.038851 -0.37359 0.045442 -0.17169 -0.54477 -0.091772 -0.32952 -0.25522 0.087106 0.048685 -0.31684 -0.064198 0.044885 -0.49587 0.429 0.36198 -0.14682 0.20122 -0.030038 0.18736 0.31626 0.059023 -0.13473 -0.40664 -0.52983 0.072459 0.082531 -0.32495 -0.006691 0.43564 -0.12976 -0.26293 0.15975 0.25062 -0.064732 -0.30689 0.16084 0.4583 0.38565 0.12612 -0.00080485 0.18055 0.24757 0.5568 0.21701 0.33105 0.11991 0.0027257 -0.10679 -0.16922 0.25277 0.36024 -0.4762 -0.20035 0.38473 -0.71653 -0.13788 -0.12201 -0.16979 -0.29624 -0.010344 0.19216 0.063375 0.30977 -0.19759 0.57419 0.34018 -0.34795 -0.085304 -0.028243 -0.39182 0.23797 -0.092997 0.11981 -0.2368 0.098179 0.083047 0.12652 0.16026 -0.14166 0.14296 -0.12868 0.18181 -0.42337 0.18061 -0.051635 -0.17987 -0.097071 0.0013755 -0.42539 0.56817 0.057432 0.31577 0.18918 0.27639 -0.39906 0.18594 -0.54231 0.61376 0.37921 0.1366 -0.39842 0.58557 0.054558 0.061801 0.32546 -0.19615 -0.034329 -0.013225 0.67854 -0.16672 0.92439 0.11952 0.040351 0.35368 -0.25573 0.26648 -0.020954 0.18535 0.062376 -0.22976 -0.074563 -0.3289 0.28535 0.46959 -0.54324 0.14124 0.19964 0.1541 0.024155 0.144 0.30989 -0.15989 -0.11611 0.09102 -0.50317 -0.33662 0.059168 0.43838 -0.1428 -0.0053718 0.046505 0.21546 0.065006 -0.35175 -0.17137 0.20467 -0.07173 0.40879 0.20295 -0.014211 -0.21898 -0.12831 0.3224 0.17608 -0.60267 0.036737 0.54848 -0.47682 -0.56556 -0.083633 0.032302 -0.25262 0.39481 -0.019623 0.62547 -0.11369 -0.25727 0.073363 0.18437 0.14587 0.32708 -0.52049 0.037555 0.023667 -0.068237 -0.22916 0.017755 -0.18394 0.55107 -0.23965 0.39187 -0.017785 0.43113 0.27181 -0.16043 -0.347 -2.4194 -0.028952 0.95085 0.05804 -0.23623 0.18914 0.31192 0.23064 -0.30309 -0.18603 0.07618 0.37337 -0.14444 -0.028793 -0.012806 -0.59707 0.31734 -0.25267 0.54384 0.063007 -0.049795 -0.16043 0.046744 -0.070621\\n'\n",
      "b'and 0.038466 -0.039792 0.082747 -0.38923 -0.21431 0.1702 -0.025657 0.09578 0.2386 -1.6342 0.14332 -0.037958 -0.019583 0.38494 0.097319 0.29697 -0.34523 0.11742 -0.024189 0.16013 0.09824 0.12811 -0.17482 0.20976 -0.22362 -0.20656 0.24428 0.066875 -0.12594 -0.015706 0.064986 0.4754 -0.055405 0.54286 -0.75188 -0.083218 0.17896 0.073084 -0.3033 -0.17416 -0.17147 -0.1192 0.038308 -0.2066 0.088679 -0.055993 0.361 0.38658 -0.055434 0.097699 0.3686 -0.326 0.13023 -0.29897 -0.24709 0.051869 0.030422 0.18586 -0.046117 -0.14765 0.35895 0.10094 -0.087822 -0.17514 -0.25403 -0.35855 0.15801 -0.027074 0.12565 -0.17509 -0.13126 -0.13916 0.053628 -0.049429 0.051938 -0.048684 0.071719 0.080952 -0.20018 -0.10871 -0.26707 -0.35727 0.3712 0.016709 -0.034959 -0.047711 0.0024827 0.10847 0.0089053 -0.14874 0.046014 0.42702 -0.24684 0.12193 -0.27579 0.25844 -0.20991 -0.086667 0.14767 -0.17441 0.17054 -0.30868 -0.08797 -0.17195 -0.11743 0.12146 0.069268 0.13311 -0.13565 -0.24855 -0.0026393 -0.71169 -0.32594 -0.36397 0.053331 0.35714 -0.30035 0.041583 -0.11996 -0.02368 0.016728 0.15869 -0.16491 -0.2782 -0.13788 0.10178 -0.24177 0.096347 0.2367 0.20885 -0.28867 0.10772 0.15562 -0.012284 -0.22161 -0.1017 0.1257 -0.29579 -0.0089604 0.35075 0.020088 0.020389 0.0038884 0.31869 -0.19848 0.060458 0.28148 -0.23499 -0.17035 -0.22323 0.57975 0.26464 -0.10124 0.081058 0.14029 0.066431 -0.12212 0.040034 0.027548 -0.1476 0.31438 -0.23046 0.2645 -0.10945 -0.39033 0.1266 -0.030951 0.067389 0.16807 0.22373 0.13502 0.38235 -0.52807 0.54013 -0.043062 0.093211 0.045211 -0.24291 -0.49781 0.26425 0.0264 0.14347 0.11763 -0.011614 0.097932 -0.26632 -0.22143 0.25156 0.08128 0.10937 -0.12199 0.019255 -0.33463 -0.18181 0.064724 0.22921 -0.032425 -0.27295 0.31983 0.16134 0.93692 -0.1214 -0.012617 0.25274 0.24615 0.13214 0.16092 0.20576 -0.051567 -0.3784 0.19761 0.16993 -0.087151 0.026922 0.38035 0.083349 0.24715 -0.1094 0.15459 -0.051741 0.16604 -0.21335 -0.030744 -0.14574 -0.50462 0.34825 -0.12343 0.17733 0.2857 -0.30467 -0.15095 0.30346 -0.15678 0.064804 -0.073008 0.26499 0.16312 0.11889 -0.63938 0.15598 -0.23643 0.59644 0.38748 0.3358 -0.58647 0.12584 0.36144 -0.33622 0.38128 -0.10348 0.18825 -0.33686 0.0058178 -0.1345 0.55511 -0.056443 0.15094 -0.28438 0.025488 0.20392 0.052712 -0.45719 0.089267 0.095469 -0.19022 -0.17101 -0.37599 -0.182 -0.065605 -0.061388 -0.19467 -0.070368 -0.23977 0.39253 -0.21283 0.17221 -1.867 -0.22609 0.53976 -0.3358 -0.48587 -0.050246 0.31694 -0.15536 0.12244 0.38356 -0.1389 0.41886 0.23664 -0.31113 0.045194 -0.20405 -0.21097 -0.11025 0.021766 0.44129 0.32797 -0.33427 0.011807 0.059703\\n'\n"
     ]
    }
   ],
   "source": [
    "# There are multiple files with different dimensionality of the features in the zip archive: 50d, 100d, 200d, 300d\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        print(line)\n",
    "        if idx == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack GloVe embeddings from a zip file, build a word-to-index dictionary, and store each word's embedding vector in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the downloaded file\n",
    "word_to_index = dict()\n",
    "embeddings = []\n",
    "\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        features = torch.tensor([float(value) for value in values[1:]])\n",
    "        word_to_index[word] = idx\n",
    "        embeddings.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the GloVe embeddings, create a word-to-index dictionary, and store each word's embedding vector in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: 300\n",
      "Padding token id: 400001\n",
      "Unknown token id: 400000\n"
     ]
    }
   ],
   "source": [
    "# Last token in the vocabulary is '<unk>' which is used for out-of-vocabulary words\n",
    "# We also add a '<pad>' token to the vocabulary for padding sequences\n",
    "word_to_index[\"<pad>\"] = len(word_to_index)\n",
    "padding_token_id = word_to_index[\"<pad>\"]\n",
    "unk_token_id = word_to_index[\"<unk>\"]\n",
    "\n",
    "embeddings.append(torch.zeros(embeddings[0].shape))\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.size(1)}\")\n",
    "print(f\"Padding token id: {padding_token_id}\")\n",
    "print(f\"Unknown token id: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to map labels to indices and vice versa, and print the number of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-RRB-', 'VBZ', 'VB', 'NNP', 'LS', ':', 'VBG', 'WRB', ')', 'POS', 'TO', 'NN', 'JJ', 'FW', 'JJS', 'RP', 'IN', 'WP', 'PRP$', \"''\", 'SYM', '-LRB-', ',', '.', '$', 'PRP', 'WDT', '#', '-NONE-', 'NNS', 'JJR', '(', 'WP$', 'UH', 'VBD', 'NNPS', '``', 'CD', 'CC', 'VBP', 'EX', 'MD', 'RBS', 'DT', 'RBR', 'RB', 'VBN', 'PDT']\n",
      "Number of classes: 48\n",
      "{'-RRB-': 0, 'VBZ': 1, 'VB': 2, 'NNP': 3, 'LS': 4, ':': 5, 'VBG': 6, 'WRB': 7, ')': 8, 'POS': 9, 'TO': 10, 'NN': 11, 'JJ': 12, 'FW': 13, 'JJS': 14, 'RP': 15, 'IN': 16, 'WP': 17, 'PRP$': 18, \"''\": 19, 'SYM': 20, '-LRB-': 21, ',': 22, '.': 23, '$': 24, 'PRP': 25, 'WDT': 26, '#': 27, '-NONE-': 28, 'NNS': 29, 'JJR': 30, '(': 31, 'WP$': 32, 'UH': 33, 'VBD': 34, 'NNPS': 35, '``': 36, 'CD': 37, 'CC': 38, 'VBP': 39, 'EX': 40, 'MD': 41, 'RBS': 42, 'DT': 43, 'RBR': 44, 'RB': 45, 'VBN': 46, 'PDT': 47}\n",
      "{0: '-RRB-', 1: 'VBZ', 2: 'VB', 3: 'NNP', 4: 'LS', 5: ':', 6: 'VBG', 7: 'WRB', 8: ')', 9: 'POS', 10: 'TO', 11: 'NN', 12: 'JJ', 13: 'FW', 14: 'JJS', 15: 'RP', 16: 'IN', 17: 'WP', 18: 'PRP$', 19: \"''\", 20: 'SYM', 21: '-LRB-', 22: ',', 23: '.', 24: '$', 25: 'PRP', 26: 'WDT', 27: '#', 28: '-NONE-', 29: 'NNS', 30: 'JJR', 31: '(', 32: 'WP$', 33: 'UH', 34: 'VBD', 35: 'NNPS', 36: '``', 37: 'CD', 38: 'CC', 39: 'VBP', 40: 'EX', 41: 'MD', 42: 'RBS', 43: 'DT', 44: 'RBR', 45: 'RB', 46: 'VBN', 47: 'PDT'}\n"
     ]
    }
   ],
   "source": [
    "labels_unique = list(set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]]))\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to tokenize text, map tokens and labels to indices, and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 13054/13054 [00:00<00:00, 15051.06 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1451/1451 [00:00<00:00, 1908.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['words', 'labels', 'token_ids', 'label_ids'])\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def map_token_to_index(token):\n",
    "    # Return the index of the token or the index of the '<unk>' token if the token is not in the vocabulary\n",
    "    return word_to_index.get(token, unk_token_id)\n",
    "\n",
    "\n",
    "def map_text_to_indices(text: str):\n",
    "    return [map_token_to_index(token.lower()) for token in text]\n",
    "\n",
    "\n",
    "def map_labels_to_indices(labels: list):\n",
    "    #return [ctoi[label] for label in labels]\n",
    "    # TODO: Implement the mapping of the labels to indices\n",
    "    return NotImplementedError\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    #return map(lambda x: {\"token_ids\": map_text_to_indices(x[\"words\"])}, dataset)\n",
    "    dataset = dataset.map(lambda x: {\"token_ids\": map_text_to_indices(x[\"words\"])}, num_proc=4)\n",
    "    dataset = dataset.map(lambda x: {\"label_ids\": map_labels_to_indices(x[\"labels\"])}, num_proc=4)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_train_tokenized = prepare_dataset(dataset[\"train\"])\n",
    "dataset_valid_tokenized = prepare_dataset(dataset[\"test\"])\n",
    "\n",
    "# Print the first sample in the tokenized training dataset\n",
    "print(dataset_train_tokenized[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text, map tokens and labels to indices, and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1627,   3637,   5949,     32,    287, 400000,      4,     30,   3479,\n",
      "              1,      0,   1557,      5,   1431,   1296,      2, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [ 19743,  17047,     14,      7,   1207,      3,    387,      9,  19743,\n",
      "         112810,      2, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [    63,     14,     80,  43144,     93, 400000,      3,    689,    281,\n",
      "           1947,      1,      0,    560,     13,     42, 400000,     14,     36,\n",
      "           1698,      4,      0,    347,   2488,   3311,    693,      2, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [     0,    365,    747,      1,    212,      1,     14,     12,     63,\n",
      "              9,     84,   4033,     12,     37,  13444,     43,     30,   1303,\n",
      "         400000,      4,   5075,     46,      4,   1603,      2, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [  1074,   1964, 400000,   2230,      1,     29,    616,     22,    614,\n",
      "          99695,   2693,   2916,    231,      1,     15,    564, 400000,     90,\n",
      "         400000,      5,    276,    616,    941, 400000,      3,  16798,   1383,\n",
      "           1018,      5,     47,   2965,   1477,   1207,      1,   9410,    347,\n",
      "            231,      2],\n",
      "        [     6,     47,    993,   7663,      3,    883,   2381,      1,   7974,\n",
      "              9,    755,    270,    238,     12, 400000,   1947,   2595,      7,\n",
      "            477,    498,      3,   7613,    262,      6,    478,    110,  10632,\n",
      "           1947,    896,      7,   2563,    498,      3,   3166,    262,      2,\n",
      "         400001, 400001],\n",
      "        [    17,      0,  66886,    263, 400000,      0,  13909,     12,     77,\n",
      "              3,   1015,    491,      9,    882,    479,     32,   3048, 400000,\n",
      "              4,   2188,      0,  14063,   1164,   1767,   1386,  13697,      4,\n",
      "           1179,     44,   1218,  44775,      2, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [  4509,     33,   3369,   2466,      9,    452,    113,      3,      0,\n",
      "          19628,      9,   4763,      4,      0,  10380,  10632,    211,      2,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001]])\n",
      "tensor([[    12,     11,     29,     39,     46,     28,     10,      2,     11,\n",
      "             22,     43,     11,     38,     11,     11,     23, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [     3,      3,      1,     43,     11,     16,      3,      9,      3,\n",
      "              3,     23, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [    40,      1,     24,     37,     37,     28,     16,     12,     11,\n",
      "             29,     22,     43,     11,     16,     26,     28,      1,     45,\n",
      "             12,     10,     43,     12,     11,     11,     11,     23, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [    43,     12,     11,     22,     45,     22,      1,     16,     40,\n",
      "              1,     43,     11,     16,     43,     11,     41,      2,     46,\n",
      "             28,     10,      3,     38,     10,     29,     23, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [     3,      3,      3,      3,     22,     43,     11,     16,      3,\n",
      "             12,     37,      3,      3,     22,     34,     46,     28,     11,\n",
      "             28,     38,     11,     12,     11,     28,     16,      3,      3,\n",
      "              3,     38,     18,     12,      6,     11,     22,      3,      3,\n",
      "              3,     23],\n",
      "        [    16,     18,     14,     11,     16,     11,     29,     22,      3,\n",
      "              9,     35,      3,     34,     16,     12,     29,     34,     43,\n",
      "             12,     11,     16,     37,     11,     16,      3,     16,     11,\n",
      "             29,     34,     43,     12,     11,     16,     37,     11,     23,\n",
      "         400001, 400001],\n",
      "        [    16,     43,     29,     34,     28,     43,     11,     16,     43,\n",
      "             16,      3,      3,      9,     14,     29,     39,      6,     28,\n",
      "             10,      2,     43,     12,     11,     11,     46,     28,     10,\n",
      "              2,     18,     29,     45,     23, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001],\n",
      "        [    29,     39,     46,      3,      9,     11,     16,     16,     43,\n",
      "             11,      9,     11,     10,     43,     12,     11,     11,     23,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
      "         400001, 400001]])\n"
     ]
    }
   ],
   "source": [
    "def pad_inputs(batch, keys_to_pad=[\"token_ids\", \"label_ids\"], padding_value=-1):\n",
    "    # Pad keys_to_pad to the maximum length in batch\n",
    "    padded_batch = {}\n",
    "    for key in keys_to_pad:\n",
    "        # Get maximum length in batch\n",
    "        max_len = max([len(sample[key]) for sample in batch])\n",
    "        # Pad all samples to the maximum length\n",
    "        padded_batch[key] = torch.tensor(\n",
    "            [\n",
    "                sample[key] + [padding_value] * (max_len - len(sample[key]))\n",
    "                for sample in batch\n",
    "            ]\n",
    "        )\n",
    "    # Add remaining keys to the batch\n",
    "    for key in batch[0].keys():\n",
    "        if key not in keys_to_pad:\n",
    "            padded_batch[key] = torch.tensor([sample[key] for sample in batch])\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32, shuffle=False):\n",
    "    # Create a DataLoader for the dataset\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(pad_inputs, padding_value=padding_token_id),\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "\n",
    "# We select the columns that we want to keep in the dataset\n",
    "dataset_train_tokenized = dataset_train_tokenized.with_format(\n",
    "    columns=[\"token_ids\", \"label_ids\"]\n",
    ")\n",
    "\n",
    "dataset_valid_tokenized = dataset_train_tokenized = dataset_valid_tokenized.with_format(\n",
    "    columns=[\"token_ids\", \"label_ids\"]\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "dataloader_train = get_dataloader(dataset_train_tokenized, batch_size=8, shuffle=True)\n",
    "dataloader_valid = get_dataloader(dataset_valid_tokenized, batch_size=8, shuffle=True)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    token_ids = batch[\"token_ids\"]\n",
    "    labels = batch[\"label_ids\"]\n",
    "    print(token_ids)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set up model, loss and optimizer\n",
    "- Cross Entropy is Softmax + Negative Log Likelihood\n",
    "- As optimizer we use Adam (adapts the learning rate per weight)\n",
    "\n",
    "(run this only once as Jupyter keeps the model (including the weights) and the optimizer in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up model and optimizer\n",
    "model = Net(embedding_vectors=embeddings, num_classes=len(ctoi), hidden_dim=embeddings.size(1)//2).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "metric_dict = {'loss': '------', 'accuracy': '------'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation function comparing prediction with gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_iter, net):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # extract input and labels\n",
    "        \n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # predict only\n",
    "        with torch.no_grad():\n",
    "            outputs = net(token_ids)\n",
    "        outputs_classes = outputs.argmax(dim=2)\n",
    "\n",
    "        # compute amount of correct predictions\n",
    "        # sequence lengths within the batch might be different, so we need to take care of that\n",
    "        inputs_lengths = (token_ids != 400001).sum(dim=1)\n",
    "        \n",
    "        total_count += inputs_lengths.sum()\n",
    "        # iterate over each sample of the batch\n",
    "        batch_size = outputs_classes.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(inputs_lengths[i]):\n",
    "                correct_count += int(outputs_classes[i][j] == labels[i][j])\n",
    "    return correct_count/total_count.float().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The actual training loop\n",
    "\n",
    "- runs several epochs\n",
    "- in each epoch\n",
    " - forward the batch\n",
    " - computes the loss for the output of the whole batch\n",
    " - reduces (e.g. average, sum) the loss\n",
    " - computes derivatives of weights by backpropagation\n",
    " - optimizer updates weights\n",
    " - evaluate on validation/development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: : 7255it [00:50, 142.98it/s, loss=0.185, accuracy=91.90%] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# a nice progress bar to make the waiting time much better\n",
    "pbar = tqdm(total=NUM_EPOCHS*len(dataloader_train), postfix=metric_dict)\n",
    "\n",
    "# run for NUM_EPOCHS epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # run for every data (in batches) of our iterator\n",
    "    \n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        # extract input and labels\n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(token_ids)\n",
    "        \n",
    "        # 2D loss function expects input as (batch, prediction, sequence) and target as (batch, sequence) containing the class index\n",
    "        loss = criterion(outputs.permute(0,2,1), labels)\n",
    "        # otherwise use view function to get rid of sequence dimension by effectively concatenating all sequence items\n",
    "        # loss = criterion(outputs.view(-1, len(classes)), labels.view(-1))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        pbar.update(labels.size(0))\n",
    "        metric_dict.update({'loss': f'{loss.item():6.3f}'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "    # evaluate on validation set after each epoch\n",
    "    metric_dict.update({'accuracy': f'{100*evaluate(dataloader_valid, model):6.2f}%'})\n",
    "    pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomly predict sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def map_list(list_: list, mapping: dict):\n",
    "    return [mapping[item] for item in list_]\n",
    "\n",
    "def tokens_to_index(tokens: list, vocabulary: dict):\n",
    "    return map_list(tokens, vocabulary)\n",
    "\n",
    "def indices_to_class(indices: list, classes: dict):\n",
    "    return map_list(indices, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This year it is expected *-1 to be a net importer and is said *-1 to be seeking *-2 to buy about 200,000 tons of sugar *-3 to meet internal needs , analysts said 0 *T*-4 .\n",
      "Prediction:   ['DT', 'NN', 'PRP', 'VBZ', 'VBN', '-NONE-', 'TO', 'VB', 'DT', 'NN', 'NN', 'CC', 'VBZ', 'VBD', '-NONE-', 'TO', 'VB', 'VBG', '-NONE-', 'TO', 'VB', 'IN', 'CD', 'NNS', 'IN', 'NN', '-NONE-', 'TO', 'VB', 'JJ', 'VBZ', ',', 'NNS', 'VBD', '-NONE-', '-NONE-', '.']\n",
      "Ground truth: ['DT', 'NN', 'PRP', 'VBZ', 'VBN', '-NONE-', 'TO', 'VB', 'DT', 'JJ', 'NN', 'CC', 'VBZ', 'VBN', '-NONE-', 'TO', 'VB', 'VBG', '-NONE-', 'TO', 'VB', 'IN', 'CD', 'NNS', 'IN', 'NN', '-NONE-', 'TO', 'VB', 'JJ', 'NNS', ',', 'NNS', 'VBD', '-NONE-', '-NONE-', '.']\n",
      "Accuracy: 91.89%\n"
     ]
    }
   ],
   "source": [
    "dataset_valid_tokenized = prepare_dataset(dataset[\"test\"])\n",
    "sample_idx = random.randint(1, len(dataset_valid_tokenized))\n",
    "sample = dataset_valid_tokenized[sample_idx]\n",
    "# map tokens to index using vocabulary\n",
    "# sample_tokens_indexed = tokens_to_index(sample.Text, vocab)\n",
    "# build input vector and add batch dimension\n",
    "sample_tensor = torch.tensor(sample[\"token_ids\"]).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(sample_tensor).squeeze(dim=0)\n",
    "\n",
    "predictions = [itoc[output.argmax(dim=0).item()] for output in outputs]\n",
    "print(\"Input:\", ' '.join(sample[\"words\"]))\n",
    "print(f\"Prediction:   {predictions}\")\n",
    "print(f\"Ground truth: {sample[\"labels\"]}\")\n",
    "accuracy = sum([1 for pred, gt in zip(predictions, sample[\"labels\"]) if pred == gt]) / len(sample[\"labels\"])\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interactive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove and add as a task\n",
    "\n",
    "# text = input(\"Please enter your text: \")\n",
    "\n",
    "# # map tokens to index using vocabulary\n",
    "# tokens = tokenizer(text)\n",
    "# tokens_indexed = tokens_to_index(tokens, vocab)\n",
    "# # build input vector and add batch dimension\n",
    "# tensor = torch.tensor(tokens_indexed).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# # forward / predict\n",
    "# with torch.no_grad():\n",
    "#     # get rid of batch dimension (is set to 1)\n",
    "#     outputs = model(tensor).squeeze(dim=0)\n",
    "\n",
    "# print(\"Input:\", tokens)\n",
    "# print(\"Prediction:\", indices_to_class(outputs.argmax(dim=1), classes))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "vqa-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
