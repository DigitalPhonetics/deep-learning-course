{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch POS Tagging\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- tqdm\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "# %conda install spacy # or install using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# download resources for english\n",
    "# `run` has to be replaced by `python` if run in a shell\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.1.2\n",
      "Torchtext Version:  0.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)\n",
    "print(\"Torchtext Version: \", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_CACHE = os.path.expanduser(\"./glove/\")\n",
    "DATASET_ROOT = os.path.expanduser(\"./\")\n",
    "BATCH_SIZE = 16 # make sure that batches fit into your device's memory but note that the batch size influences your training (it is a hyperparameter)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 'cuda' for GPU (optional specify device id) and 'cpu' for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The dataset is adapted from the UDPOS where the format has been slightly changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Package `portalocker` is required to be installed to use this datapipe.Please use `pip install 'portalocker>=2.0.0'` or`conda install -c conda-forge 'portalocker>=2/0.0'`to install the package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:38\u001b[0m, in \u001b[0;36m_assert_portalocker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mportalocker\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'portalocker'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UDPOS\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m----> 5\u001b[0m train_datapipe \u001b[38;5;241m=\u001b[39m \u001b[43mUDPOS\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dev_datapipe \u001b[38;5;241m=\u001b[39m UDPOS(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[0;34m(root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(new_root):\n\u001b[1;32m    192\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(new_root, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m _check_default_set(split, splits, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_datasets(\u001b[38;5;28mtuple\u001b[39m(result), split)\n",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torchtext/datasets/udpos.py:71\u001b[0m, in \u001b[0;36mUDPOS\u001b[0;34m(root, split)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m url_dp \u001b[38;5;241m=\u001b[39m IterableWrapper([URL])\n\u001b[0;32m---> 71\u001b[0m cache_compressed_dp \u001b[38;5;241m=\u001b[39m \u001b[43murl_dp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_disk_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_filepath_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m_filepath_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMD5\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m cache_compressed_dp \u001b[38;5;241m=\u001b[39m HttpReader(cache_compressed_dp)\u001b[38;5;241m.\u001b[39mend_caching(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, same_filepath_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m cache_decompressed_dp \u001b[38;5;241m=\u001b[39m cache_compressed_dp\u001b[38;5;241m.\u001b[39mon_disk_cache(filepath_fn\u001b[38;5;241m=\u001b[39mpartial(_extracted_filepath_fn, root, split))\n",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torch/utils/data/datapipes/datapipe.py:141\u001b[0m, in \u001b[0;36mIterDataPipe.register_datapipe_as_function.<locals>.class_function\u001b[0;34m(cls, enable_df_api_tracing, source_dp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclass_function\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enable_df_api_tracing, source_dp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 141\u001b[0m     result_pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource_dp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_pipe, IterDataPipe):\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m enable_df_api_tracing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_dp, DFIterDataPipe):\n",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:208\u001b[0m, in \u001b[0;36mOnDiskCacheHolderIterDataPipe.__init__\u001b[0;34m(self, source_datapipe, filepath_fn, hash_dict, hash_type, extra_check_fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     source_datapipe: IterDataPipe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     extra_check_fn: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[0;32m--> 208\u001b[0m     \u001b[43m_assert_portalocker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_datapipe \u001b[38;5;241m=\u001b[39m source_datapipe\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/intro-dl-ws-2023-24/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:47\u001b[0m, in \u001b[0;36m_assert_portalocker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `portalocker` is required to be installed to use this datapipe.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportalocker>=2.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`conda install -c conda-forge \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportalocker>=2/0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto install the package\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Package `portalocker` is required to be installed to use this datapipe.Please use `pip install 'portalocker>=2.0.0'` or`conda install -c conda-forge 'portalocker>=2/0.0'`to install the package"
     ]
    }
   ],
   "source": [
    "class POSTaggingDataset(torchtext.legacy.data.TabularDataset):\n",
    "\n",
    "    # Universal Dependencies English Web Treebank by Universal Dependencies contributors\n",
    "    # Modified by Maximilian Schmidt for use at the IMS, University of Stuttgart\n",
    "    # License: http://creativecommons.org/licenses/by-sa/4.0/\n",
    "    urls = ['file:./udpos/en-ud-v2']\n",
    "    dirname = 'en-ud-v2'\n",
    "    name = 'udpos'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label1_field, label2_field, id_field, root=\".data\", train=\"train.jsonl\",\n",
    "               validation=\"dev.jsonl\",\n",
    "               test=\"test.jsonl\", **kwargs):\n",
    "        \"\"\"Downloads and loads the Universal Dependencies Version 2 POS Tagged\n",
    "        data.\n",
    "        \"\"\"\n",
    "\n",
    "        fields = {'text': text_field}\n",
    "        if label1_field is not None:\n",
    "            fields.update(label1=label1_field)\n",
    "        if label2_field is not None:\n",
    "            fields.update(label2=label2_field)\n",
    "        if id_field is not None:\n",
    "            fields.update(id=id_field)\n",
    "\n",
    "        return super(POSTaggingDataset, cls).splits(\n",
    "            fields=fields, root=root, train=train, validation=validation,\n",
    "            format='json', test=test, **kwargs)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our neural network consists of one fully connected linear layer\n",
    "\n",
    "The softmax is part of the loss function in PyTorch, so you can omit this in the forward function.\n",
    "\n",
    "The embedding layer\n",
    "- maps from indices to vectors\n",
    "- is not trained (freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    # this resembles a really simple neural network: an embedding layer followed by a fully\n",
    "    # connected linear layer such that predictions are computed for each token in the sequence\n",
    "    # and batch independently\n",
    "    def __init__(self, embedding_vectors, num_classes):\n",
    "        super().__init__()\n",
    "        # PyTorch's embedding layer maps from indices to embeddings, freeze will tell PyTorch to\n",
    "        # not train this layer, i.e. not modifying any weight\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_vectors, freeze=True)\n",
    "        # a fully connected linear layer mapping the embedded vector to a vector of fixed size\n",
    "        # (num_classes in this case)\n",
    "        self.fc = torch.nn.Linear(embedding_vectors.size(1), num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # simple forwarding through our model\n",
    "        # PyTorch takes care of keeping track of the operations for the backward pass\n",
    "        emmedded_inputs = self.embedding(inputs)\n",
    "        outputs = self.fc(emmedded_inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Set up our fields as placeholder for the actual data\n",
    "\n",
    "- text (input)\n",
    "- label (gold label / ground truth)\n",
    "\n",
    "### Split into training, validation & test dataset and build vocabulary for *training* dataset (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up fields\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True, lower=True, include_lengths=True, batch_first=True, tokenize='spacy')\n",
    "LABEL = torchtext.legacy.data.Field(sequential=True, use_vocab=True, batch_first=True, unk_token=None)\n",
    "\n",
    "# make splits for data\n",
    "train, val, test = POSTaggingDataset.splits(root=DATASET_ROOT, text_field=('Text',TEXT), label1_field=None, label2_field=('Label',LABEL), id_field=None)\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300, cache=EMB_CACHE))\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GloVe\n",
    "GloVe embeddings were trained with a special objective.\n",
    "Word pairs share the same underlying concept: Vector differences should be roughly equal.\n",
    "\n",
    "<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\" width=500/>\\\n",
    "source: https://nlp.stanford.edu/projects/glove/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create iterator such that each iteration returns a batch from shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available classes: 51\n",
      "['<pad>', 'NN', 'IN', 'DT', 'NNP', 'PRP', 'JJ', 'RB', '.', 'VB', 'NNS', ',', 'CC', 'VBP', 'VBD', 'VBZ', 'CD', 'VBN', 'VBG', 'MD', 'TO', 'PRP$', '-RRB-', '-LRB-', 'WDT', 'WRB', ':', 'WP', 'UH', '``', \"''\", 'RP', 'HYPH', 'POS', 'NNPS', 'JJR', 'JJS', 'NFP', 'EX', 'ADD', 'GW', 'RBR', '$', 'PDT', 'RBS', 'SYM', 'FW', 'LS', 'AFX', 'WP$', 'XX']\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = torchtext.legacy.data.Iterator.splits((train, val, test), batch_size=BATCH_SIZE, device=DEVICE, sort=False)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "classes = LABEL.vocab.itos\n",
    "print(f\"Available classes: {len(classes)}\\n{classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set up model, loss and optimizer\n",
    "- Cross Entropy is Softmax + Negative Log Likelihood\n",
    "- As optimizer we use Adam (adapts the learning rate per weight)\n",
    "\n",
    "(run this only once as Jupyter keeps the model (including the weights) and the optimizer in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up model and optimizer\n",
    "model = Net(vocab.vectors, len(classes)).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "metric_dict = {'loss': '------', 'accuracy': '------'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation function comparing prediction with gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_iter, net):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # extract input and labels\n",
    "        (inputs, inputs_lengths), labels = batch.Text, batch.Label\n",
    "\n",
    "        # predict only\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs)\n",
    "        outputs_classes = outputs.argmax(dim=2)\n",
    "\n",
    "        # compute amount of correct predictions\n",
    "        # sequence lengths within the batch might be different, so we need to take care of that\n",
    "\n",
    "        total_count += inputs_lengths.sum()\n",
    "        # iterate over each sample of the batch\n",
    "        batch_size = outputs_classes.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(inputs_lengths[i]):\n",
    "                correct_count += int(outputs_classes[i][j] == labels[i][j])\n",
    "    return correct_count/total_count.float().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The actual training loop\n",
    "\n",
    "- runs several epochs\n",
    "- in each epoch\n",
    " - forward the batch\n",
    " - computes the loss for the output of the whole batch\n",
    " - reduces (e.g. average, sum) the loss\n",
    " - computes derivatives of weights by backpropagation\n",
    " - optimizer updates weights\n",
    " - evaluate on validation/development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# a nice progress bar to make the waiting time much better\n",
    "pbar = tqdm(total=NUM_EPOCHS*len(train), postfix=metric_dict)\n",
    "\n",
    "# run for NUM_EPOCHS epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # run for every data (in batches) of our iterator\n",
    "    \n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # extract input and labels\n",
    "        (inputs, inputs_lengths), labels = batch.Text, batch.Label\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2D loss function expects input as (batch, prediction, sequence) and target as (batch, sequence) containing the class index\n",
    "        loss = criterion(outputs.permute(0,2,1), labels)\n",
    "        # otherwise use view function to get rid of sequence dimension by effectively concatenating all sequence items\n",
    "        # loss = criterion(outputs.view(-1, len(classes)), labels.view(-1))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        pbar.update(labels.size(0))\n",
    "        metric_dict.update({'loss': f'{loss.item():6.3f}'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "    # evaluate on validation set after each epoch\n",
    "    metric_dict.update({'accuracy': f'{100*evaluate(val_iter, model):6.2f}%'})\n",
    "    pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomly predict sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def map_list(list_: list, mapping: dict):\n",
    "    return [mapping[item] for item in list_]\n",
    "\n",
    "def tokens_to_index(tokens: list, vocabulary: dict):\n",
    "    return map_list(tokens, vocabulary)\n",
    "\n",
    "def indices_to_class(indices: list, classes: dict):\n",
    "    return map_list(indices, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get any tokenizer\n",
    "tokenizer = torchtext.data.get_tokenizer('spacy', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t\t     one little boy stands up and offers that , \" if my best friend who lives next door was playing in the street when a car came along and killed him , that would be a tragedy \" .\n",
      "Prediction:\t     ['CD', 'JJ', 'NN', 'VBZ', 'RP', 'CC', 'NN', 'DT', ',', \"''\", 'IN', 'PRP$', 'JJS', 'NN', 'WP', 'NNS', 'NN', 'NN', 'VBD', 'VBG', 'IN', 'DT', 'NN', 'WRB', 'DT', 'NN', 'VBD', 'RB', 'CC', 'NNP', 'PRP', ',', 'DT', 'MD', 'VB', 'DT', 'NN', \"''\", '.']\n",
      "Expected prediction: ['CD', 'JJ', 'NN', 'VBZ', 'RB', 'CC', 'VBZ', 'IN', ',', '``', 'IN', 'PRP$', 'JJS', 'NN', 'WP', 'VBZ', 'JJ', 'NN', 'VBD', 'VBG', 'IN', 'DT', 'NN', 'WRB', 'DT', 'NN', 'VBD', 'RB', 'CC', 'VBD', 'PRP', ',', 'DT', 'MD', 'VB', 'DT', 'NN', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "sample_idx = random.randint(1, len(test))\n",
    "sample = test[sample_idx]\n",
    "# map tokens to index using vocabulary\n",
    "sample_tokens_indexed = tokens_to_index(sample.Text, vocab)\n",
    "# build input vector and add batch dimension\n",
    "sample_tensor = torch.tensor(sample_tokens_indexed).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(sample_tensor).squeeze(dim=0)\n",
    "\n",
    "print(\"Input:\\t\\t    \", ' '.join(sample.Text))\n",
    "print(\"Prediction:\\t    \", indices_to_class(outputs.argmax(dim=1), classes))\n",
    "print(\"Expected prediction:\", sample.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interactive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your text: Our class today takes place online since there is a strike in public transport.\n",
      "Input: ['Our', 'class', 'today', 'takes', 'place', 'online', 'since', 'there', 'is', 'a', 'strike', 'in', 'public', 'transport', '.']\n",
      "Prediction: ['<pad>', 'NN', 'NN', 'VBZ', 'NN', 'NN', 'IN', 'EX', 'VBZ', 'DT', 'NN', 'IN', 'JJ', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Please enter your text: \")\n",
    "\n",
    "# map tokens to index using vocabulary\n",
    "tokens = tokenizer(text)\n",
    "tokens_indexed = tokens_to_index(tokens, vocab)\n",
    "# build input vector and add batch dimension\n",
    "tensor = torch.tensor(tokens_indexed).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(tensor).squeeze(dim=0)\n",
    "\n",
    "print(\"Input:\", tokens)\n",
    "print(\"Prediction:\", indices_to_class(outputs.argmax(dim=1), classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "68c77cae-cd27-403c-9dfb-47426a2e6b55",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Logging Tools: Tensorboard\n",
    "\n",
    "* [Tensorboard](https://www.tensorflow.org/tensorboard) is an extremely useful logging tool originally developed for the tensorflow framework\n",
    "* It provides a webserver to log and display (live!)\n",
    "    * scalar plots\n",
    "    * image data\n",
    "    * distributions \n",
    "    * histograms\n",
    "    * model graphs\n",
    "* Logging e.g. gradients / network weights can help you find bugs in your training process\n",
    "* Pytorch now contains bindings to this tool as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bcbba8d2-736d-44d8-ba39-f9e1172917d9",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Tensorboard Example](images/tensorboard_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0d42f576-9cfe-4b51-b5a6-baefca759714",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* To install the package, execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8389c267-e48b-4732-91fd-8392b4cdb6ff",
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install tensorboard\n",
    "# %conda install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 1.9 needs setuptools<=59.5.0 for tensorboard to work.\\\n",
    "In this case run the following (note that `<` is escaped here as it refers to input redirection for the magic command otherwise; you don't need `\\` if it is run as a command in your shell (i.e. not as a magic command)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"setuptools<=59.5.0\"\n",
    "# %conda install \"setuptools\\<=59.5.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2e784b5e-f0b8-4443-97ec-37ed41eef864",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* To add tensorboard logging to your training routines, you only need to add a `torch.utils.tensorboard.SummaryWriter` instance\n",
    "* You can add data by calling `add_*`-functions on the `SummaryWriter` instance\n",
    "    * e.g. in the picture above (scalar plots): `add_scalar`\n",
    "* You can specify a log directory via the `log_dir` parameter in the `SummaryWriter`'s constructor\n",
    "    * if none specified, a default folder called `runs` will be created in your execution directory\n",
    "* Each time you create an instance of `SummaryWriter`, a new file will be added to the log directory\n",
    "    * This prevents overwriting your old logs by accident\n",
    "    * Each file will be displayed in the column to the left (underneath the `Runs`-section)\n",
    "    * Individual log files can be toggled as not to clutter your view too much\n",
    "        * Having multiple ones active at the same time gives you an easy way to compare diffrent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "82113892-a303-4982-82c6-9d4c18873a12",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dummy training routine\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math\n",
    "\n",
    "writer = SummaryWriter()\n",
    "for epoch in range(10):\n",
    "    # ... your training code\n",
    "    epoch_accuracy = epoch/10 # dummy accuracy\n",
    "    epoch_loss = math.log10(10)-math.log10(epoch+1) # dummy loss\n",
    "\n",
    "    # Append a new data point to log \"accuracy\" to category \"train\"\n",
    "    # - the '/' token creates the grouping as you can see in the picture above: \n",
    "    #     e.g. the top group is 'epoch', the 2 graphs are called 'acc' and 'loss'\n",
    "    #          the bottom group is 'eval', the 2 graphs are called 'acc' and 'loss'\n",
    "    # Here: first value after the tag is the y axis, second value is the x axis\n",
    "    writer.add_scalar(\"train/accuracy\", epoch_accuracy, epoch)\n",
    "    writer.add_scalar(\"train/loss\", epoch_loss, epoch)\n",
    "    writer.add_scalar(\"othergroup/epoch\", epoch, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "89b46bcb-c780-4c72-ba67-8bd441867a1f",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* To visualize your logs (live) in your notebook, you can load a jupyter extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d9ac9dec-ca82-4985-92af-f6afaae8b337",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "44363a5e-1256-4da3-8302-1fddf3f1680d",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* And then include the visualization in your notebook by using (replace `runs` with the folder your logging to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "87c63f30-b06b-4879-8107-7f60b9525fed",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6b33d983-aba3-4a85-9cfb-5a74b886dbf9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "NOTE: If you want to run tensorboard as a standalone-server\n",
    "* Then start the server from the command line with the command `tensorboard --logdir runs`\n",
    "    * replace `runs` with the folder you chose to log to\n",
    "* In a webbrowser, navigate to [localhost:6006](http://localhost:6006)\n",
    "* If you're training on a remote server, you can e.g. use ssh port forwarding to run tensorboard on the server and access the webpage from your machine\n",
    "* If the default port `6006` should not be free, you can specify a different one when starting tensorboard, e.g. `tensorboard --logdir=runs  --port=6007`\n",
    "    * Change the port in the webbrowser URL accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Some alternatives\n",
    "    * [CometML](https://www.comet.ml/docs/python-sdk/pytorch/)\n",
    "    * [WandB](https://wandb.ai/site/experiment-tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Frameworks\n",
    "\n",
    "* Until now: we wrote the training and evaluation loops ourselves\n",
    "* However, a lot of the code is repetitive\n",
    "* Frameworks help you avoid repetitive code and some solve common problems like\n",
    "    * Training and evaluation loops\n",
    "    * Multi-GPU / Multi-Node training\n",
    "    * Early stopping\n",
    "    * Creating model checkpoints\n",
    "    * Hyper parameter search\n",
    "    * ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Some examples:\n",
    "    * [Pytorch Ignite](https://pytorch.org/ignite/index.html) (\"high-level library to help with training and evaluating neural networks \")\n",
    "    * [Pytorch Lightning](https://www.pytorchlightning.ai) (\"The ultimate PyTorch research framework\")\n",
    "    * [Skorch](https://github.com/skorch-dev/skorch) (\"scikit-learn compatible neural network library that wraps PyTorch\")\n",
    "    * Huggingface libraries (\"State-of-the-art Natural Language Processing\"): [Transformers](https://huggingface.co/transformers/index.html), [Datasets](https://huggingface.co/docs/datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Pytorch Ignite (taken from [website](https://pytorch.org/ignite/index.html))\n",
    "\n",
    "```python\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "model = Net()\n",
    "train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"nll\": Loss(criterion)\n",
    "}\n",
    "evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(trainer):\n",
    "    print(f\"Epoch[{trainer.state.epoch}] Loss: {trainer.state.output:.2f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(val_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
    "\n",
    "trainer.run(train_loader, max_epochs=100)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tips  & Tricks\n",
    "\n",
    "* Log the magnitude of your gradients. If you encounter problems with exploding gradients, you can try to clip the gradient values to a specific range using [torch.nn.utils.clip_grad_value_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html) or the norm over all parameters using [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_) \n",
    "* [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "* Ensembling\n",
    "* Try [more sophisticated initialisations](https://pytorch.org/docs/stable/nn.init.html), e.g. Xavier \n",
    "* [Batch normalization](https://pytorch.org/docs/stable/nn.html#normalization-layers) (can reduce dependence on initialization, higher learning rates)\n",
    "* Dataset augmentation (e.g. preprocess pictures with rotations, color shifts, mirroring, ...)\n",
    "* Look for existing datasets, architectures and pre-trained models (github, pytorch model zoo, huggingface model hub, fastai, ...)\n",
    "* Transfer-learning / fine-tuning pretrained models for your data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "vqa-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
