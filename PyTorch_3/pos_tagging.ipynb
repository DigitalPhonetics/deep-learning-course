{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch POS Tagging\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- huggingface datasets\n",
    "- tqdm\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "# %conda install spacy # or install using conda\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# download resources for english\n",
    "# `run` has to be replaced by `python` if run in a shell\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the POS tagging dataset from the Hugging Face hub and prepares it for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"batterydata/pos_tagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the loaded dataset followed by its training and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset[\"train\"])\n",
    "print(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_CACHE = os.path.expanduser(\"./glove/\")\n",
    "DATASET_ROOT = os.path.expanduser(\"./\")\n",
    "BATCH_SIZE = 16 # make sure that batches fit into your device's memory but note that the batch size influences your training (it is a hyperparameter)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 'cuda' for GPU (optional specify device id) and 'cpu' for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our neural network consists of one fully connected linear layer\n",
    "\n",
    "The softmax is part of the loss function in PyTorch, so you can omit this in the forward function.\n",
    "\n",
    "The embedding layer\n",
    "- maps from indices to vectors\n",
    "- is not trained (freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    # this resembles a really simple neural network: an embedding layer followed by a fully\n",
    "    # connected linear layer such that predictions are computed for each token in the sequence\n",
    "    # and batch independently\n",
    "    def __init__(self, embedding_vectors, num_classes):\n",
    "        super().__init__()\n",
    "        # PyTorch's embedding layer maps from indices to embeddings, freeze will tell PyTorch to\n",
    "        # not train this layer, i.e. not modifying any weight\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_vectors, freeze=True)\n",
    "        # a fully connected linear layer mapping the embedded vector to a vector of fixed size\n",
    "        # (num_classes in this case)\n",
    "        self.fc = torch.nn.Linear(embedding_vectors.size(1), num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # simple forwarding through our model\n",
    "        # PyTorch takes care of keeping track of the operations for the backward pass\n",
    "        emmedded_inputs = self.embedding(inputs)\n",
    "        outputs = self.fc(emmedded_inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GloVe\n",
    "GloVe embeddings were trained with a special objective.\n",
    "Word pairs share the same underlying concept: Vector differences should be roughly equal.\n",
    "\n",
    "<img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\" width=500/>\\\n",
    "source: https://nlp.stanford.edu/projects/glove/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create iterator such that each iteration returns a batch from shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe embeddings\n",
    "glove = hf_hub_download(\"stanfordnlp/glove\", \"glove.6B.zip\")\n",
    "\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    print(f.namelist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the 'glove.6B.300d.txt' file from the downloaded GloVe archive and print the first few lines for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple files with different dimensionality of the features in the zip archive: 50d, 100d, 200d, 300d\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        print(line)\n",
    "        if idx == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack GloVe embeddings from a zip file, build a word-to-index dictionary, and store each word's embedding vector in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the downloaded file\n",
    "word_to_index = dict()\n",
    "embeddings = []\n",
    "\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        features = torch.tensor([float(value) for value in values[1:]])\n",
    "        word_to_index[word] = idx\n",
    "        embeddings.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the GloVe embeddings, create a word-to-index dictionary, and store each word's embedding vector in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last token in the vocabulary is '<unk>' which is used for out-of-vocabulary words\n",
    "# We also add a '<pad>' token to the vocabulary for padding sequences\n",
    "word_to_index[\"<pad>\"] = len(word_to_index)\n",
    "padding_token_id = word_to_index[\"<pad>\"]\n",
    "unk_token_id = word_to_index[\"<unk>\"]\n",
    "\n",
    "embeddings.append(torch.zeros(embeddings[0].shape))\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.size(1)}\")\n",
    "print(f\"Padding token id: {padding_token_id}\")\n",
    "print(f\"Unknown token id: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to map labels to indices and vice versa, and print the number of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = list(set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]]))\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to tokenize text, map tokens and labels to indices, and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def map_token_to_index(token):\n",
    "    # Return the index of the token or the index of the '<unk>' token if the token is not in the vocabulary\n",
    "    return word_to_index.get(token, unk_token_id)\n",
    "\n",
    "\n",
    "def map_text_to_indices(text: str):\n",
    "    return [map_token_to_index(token.lower()) for token in text]\n",
    "\n",
    "\n",
    "def map_labels_to_indices(labels: list):\n",
    "    return [ctoi[label] for label in labels]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    #return map(lambda x: {\"token_ids\": map_text_to_indices(x[\"words\"])}, dataset)\n",
    "    dataset = dataset.map(lambda x: {\"token_ids\": map_text_to_indices(x[\"words\"])}, num_proc=4)\n",
    "    dataset = dataset.map(lambda x: {\"label_ids\": map_labels_to_indices(x[\"labels\"])}, num_proc=4)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_train_tokenized = prepare_dataset(dataset[\"train\"])\n",
    "dataset_valid_tokenized = prepare_dataset(dataset[\"test\"])\n",
    "\n",
    "# Print the first sample in the tokenized training dataset\n",
    "print(dataset_train_tokenized[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text, map tokens and labels to indices, and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_inputs(batch, keys_to_pad=[\"token_ids\", \"label_ids\"], padding_value=-1):\n",
    "    # Pad keys_to_pad to the maximum length in batch\n",
    "    padded_batch = {}\n",
    "    for key in keys_to_pad:\n",
    "        # Get maximum length in batch\n",
    "        max_len = max([len(sample[key]) for sample in batch])\n",
    "        # Pad all samples to the maximum length\n",
    "        padded_batch[key] = torch.tensor(\n",
    "            [\n",
    "                sample[key] + [padding_value] * (max_len - len(sample[key]))\n",
    "                for sample in batch\n",
    "            ]\n",
    "        )\n",
    "    # Add remaining keys to the batch\n",
    "    for key in batch[0].keys():\n",
    "        if key not in keys_to_pad:\n",
    "            padded_batch[key] = torch.tensor([sample[key] for sample in batch])\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32, shuffle=False):\n",
    "    # Create a DataLoader for the dataset\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(pad_inputs, padding_value=padding_token_id),\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "\n",
    "# We select the columns that we want to keep in the dataset\n",
    "dataset_train_tokenized = dataset_train_tokenized.with_format(\n",
    "    columns=[\"token_ids\", \"label_ids\"]\n",
    ")\n",
    "\n",
    "dataset_valid_tokenized = dataset_train_tokenized = dataset_valid_tokenized.with_format(\n",
    "    columns=[\"token_ids\", \"label_ids\"]\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "dataloader_train = get_dataloader(dataset_train_tokenized, batch_size=8, shuffle=True)\n",
    "dataloader_valid = get_dataloader(dataset_valid_tokenized, batch_size=8, shuffle=True)\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    token_ids = batch[\"token_ids\"]\n",
    "    labels = batch[\"label_ids\"]\n",
    "    print(token_ids)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Set up model, loss and optimizer\n",
    "- Cross Entropy is Softmax + Negative Log Likelihood\n",
    "- As optimizer we use Adam (adapts the learning rate per weight)\n",
    "\n",
    "(run this only once as Jupyter keeps the model (including the weights) and the optimizer in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up model and optimizer\n",
    "model = Net(embedding_vectors=embeddings, num_classes=len(ctoi)).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=padding_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "metric_dict = {'loss': '------', 'accuracy': '------'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation function comparing prediction with gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_iter, net):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # extract input and labels\n",
    "        \n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # predict only\n",
    "        with torch.no_grad():\n",
    "            outputs = net(token_ids)\n",
    "        outputs_classes = outputs.argmax(dim=2)\n",
    "\n",
    "        # compute amount of correct predictions\n",
    "        # sequence lengths within the batch might be different, so we need to take care of that\n",
    "        inputs_lengths = (token_ids != 400001).sum(dim=1)\n",
    "        \n",
    "        total_count += inputs_lengths.sum()\n",
    "        # iterate over each sample of the batch\n",
    "        batch_size = outputs_classes.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(inputs_lengths[i]):\n",
    "                correct_count += int(outputs_classes[i][j] == labels[i][j])\n",
    "    return correct_count/total_count.float().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The actual training loop\n",
    "\n",
    "- runs several epochs\n",
    "- in each epoch\n",
    " - forward the batch\n",
    " - computes the loss for the output of the whole batch\n",
    " - reduces (e.g. average, sum) the loss\n",
    " - computes derivatives of weights by backpropagation\n",
    " - optimizer updates weights\n",
    " - evaluate on validation/development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# a nice progress bar to make the waiting time much better\n",
    "pbar = tqdm(total=NUM_EPOCHS*len(dataloader_train), postfix=metric_dict)\n",
    "\n",
    "# run for NUM_EPOCHS epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # run for every data (in batches) of our iterator\n",
    "    \n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        # extract input and labels\n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(token_ids)\n",
    "        \n",
    "        # 2D loss function expects input as (batch, prediction, sequence) and target as (batch, sequence) containing the class index\n",
    "        loss = criterion(outputs.permute(0,2,1), labels)\n",
    "        # otherwise use view function to get rid of sequence dimension by effectively concatenating all sequence items\n",
    "        # loss = criterion(outputs.view(-1, len(classes)), labels.view(-1))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        pbar.update(labels.size(0))\n",
    "        metric_dict.update({'loss': f'{loss.item():6.3f}'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "    # evaluate on validation set after each epoch\n",
    "    metric_dict.update({'accuracy': f'{100*evaluate(dataloader_valid, model):6.2f}%'})\n",
    "    pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomly predict sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def map_list(list_: list, mapping: dict):\n",
    "    return [mapping[item] for item in list_]\n",
    "\n",
    "def tokens_to_index(tokens: list, vocabulary: dict):\n",
    "    return map_list(tokens, vocabulary)\n",
    "\n",
    "def indices_to_class(indices: list, classes: dict):\n",
    "    return map_list(indices, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset_valid_tokenized = prepare_dataset(dataset[\"test\"])\n",
    "sample_idx = random.randint(1, len(dataset_valid_tokenized))\n",
    "sample = dataset_valid_tokenized[sample_idx]\n",
    "# map tokens to index using vocabulary\n",
    "# sample_tokens_indexed = tokens_to_index(sample.Text, vocab)\n",
    "# build input vector and add batch dimension\n",
    "sample_tensor = torch.tensor(sample[\"token_ids\"]).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(sample_tensor).squeeze(dim=0)\n",
    "\n",
    "predictions = [itoc[output.argmax(dim=0).item()] for output in outputs]\n",
    "print(\"Input:\", ' '.join(sample[\"words\"]))\n",
    "print(f\"Prediction:   {predictions}\")\n",
    "print(f\"Ground truth: {sample[\"labels\"]}\")\n",
    "accuracy = sum([1 for pred, gt in zip(predictions, sample[\"labels\"]) if pred == gt]) / len(sample[\"labels\"])\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interactive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Please enter your text: \")\n",
    "\n",
    "# map tokens to index using vocabulary\n",
    "tokens = tokenizer(text)\n",
    "tokens_indexed = tokens_to_index(tokens, vocab)\n",
    "# build input vector and add batch dimension\n",
    "tensor = torch.tensor(tokens_indexed).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(tensor).squeeze(dim=0)\n",
    "\n",
    "print(\"Input:\", tokens)\n",
    "print(\"Prediction:\", indices_to_class(outputs.argmax(dim=1), classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training Frameworks\n",
    "\n",
    "* Until now: we wrote the training and evaluation loops ourselves\n",
    "* However, a lot of the code is repetitive\n",
    "* Frameworks help you avoid repetitive code and some solve common problems like\n",
    "    * Training and evaluation loops\n",
    "    * Multi-GPU / Multi-Node training\n",
    "    * Early stopping\n",
    "    * Creating model checkpoints\n",
    "    * Hyper parameter search\n",
    "    * ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Some examples:\n",
    "    * [Pytorch Ignite](https://pytorch.org/ignite/index.html) (\"high-level library to help with training and evaluating neural networks \")\n",
    "    * [Pytorch Lightning](https://www.pytorchlightning.ai) (\"The ultimate PyTorch research framework\")\n",
    "    * [Skorch](https://github.com/skorch-dev/skorch) (\"scikit-learn compatible neural network library that wraps PyTorch\")\n",
    "    * Huggingface libraries (\"State-of-the-art Natural Language Processing\"): [Transformers](https://huggingface.co/transformers/index.html), [Datasets](https://huggingface.co/docs/datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Pytorch Ignite (taken from [website](https://pytorch.org/ignite/index.html))\n",
    "\n",
    "```python\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "model = Net()\n",
    "train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"nll\": Loss(criterion)\n",
    "}\n",
    "evaluator = create_supervised_evaluator(model, metrics=val_metrics)\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(trainer):\n",
    "    print(f\"Epoch[{trainer.state.epoch}] Loss: {trainer.state.output:.2f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(val_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
    "\n",
    "trainer.run(train_loader, max_epochs=100)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tips  & Tricks\n",
    "\n",
    "* Log the magnitude of your gradients. If you encounter problems with exploding gradients, you can try to clip the gradient values to a specific range using [torch.nn.utils.clip_grad_value_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html) or the norm over all parameters using [torch.nn.utils.clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_) \n",
    "* [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "* Ensembling\n",
    "* Try [more sophisticated initialisations](https://pytorch.org/docs/stable/nn.init.html), e.g. Xavier \n",
    "* [Batch normalization](https://pytorch.org/docs/stable/nn.html#normalization-layers) (can reduce dependence on initialization, higher learning rates)\n",
    "* Dataset augmentation (e.g. preprocess pictures with rotations, color shifts, mirroring, ...)\n",
    "* Look for existing datasets, architectures and pre-trained models (github, pytorch model zoo, huggingface model hub, fastai, ...)\n",
    "* Transfer-learning / fine-tuning pretrained models for your data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "vqa-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
