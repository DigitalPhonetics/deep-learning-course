{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some Huggingface libraries for downloading and processing the dataset, and therefore need to install them first.\\\n",
    "The first is the datasets library which allows you to download and manipulate datasets.\\\n",
    "See [their guide](https://huggingface.co/docs/datasets/installation) for an introduction to the datasets library if you want to know more.\\\n",
    "The other one is the tokenizers\n",
    "We can simple install it using pip:\n",
    "> Note that `%pip` is a magic command for use in Jupyter Notebooks as discussed in the tutorial on setting up Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis (SA) determines the emotional tone of a text sequence (e.g., a sentence) and classifies it into predefined categories.\\\n",
    "Therefore, SA is a text classification task which assign a single class to the whole input.\n",
    "\n",
    "In this part, we will use the SST2 dataset, which stands for Stanford Sentiment Treebank.\\\n",
    "This is a binary task where inputs are labeled as `positive` or `negative`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "There are several possibiliites for downloading datasets.\\\n",
    "You can read them from files, or directly use a library with an online repository of datasets, such as [Huggingface Datasets](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "> A note on the Huggingface Datasets library: You can decide where downloaded files are cached, see https://huggingface.co/docs/datasets/v3.2.0/en/cache#cache-directory.\\\n",
    "> The documentation is as always a good source of information!\n",
    "\n",
    "We will use this library in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "# Print the dataset object to see the dataset's structure\n",
    "print(sst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about using this library, there is a very useful [tutorial](https://huggingface.co/docs/datasets/v3.2.0/en/tutorial) available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are three splits, and how many samples are contained in each split.\\\n",
    "Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = sst2['train']\n",
    "# Print the first sample in the training dataset\n",
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each sample includes an index, the input sentence and a label.\\\n",
    "This sample was labeled `negative` (`0`; `1` stand for `positive` in this dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "TODO \n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe embeddings\n",
    "glove = hf_hub_download(\"stanfordnlp/glove\", \"glove.6B.zip\")\n",
    "\n",
    "# Unpack the downloaded file\n",
    "word_to_indices = dict()\n",
    "embeddings = []\n",
    "# There are multiple files with different dimensionality of the features in the zip archive: 50d, 100d, 200d, 300d\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        features = np.asarray(values[1:], dtype=\"float32\")\n",
    "        word_to_indices[word] = idx\n",
    "        embeddings.append(features)\n",
    "\n",
    "# Last token in the vocabulary is '<unk>' which is used for out-of-vocabulary words\n",
    "print(f\"Number of words in the GloVe embeddings: {len(embeddings)}\")\n",
    "print(f\"Embedding shape: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def map_token_to_index(token):\n",
    "    # Return the index of the token or the index of the '<unk>' token if the token is not in the vocabulary\n",
    "    return word_to_indices.get(token, word_to_indices[\"<unk>\"])\n",
    "\n",
    "\n",
    "def map_text_to_indices(text: str):\n",
    "    return [map_token_to_index(token) for token in tokenize(text)]\n",
    "\n",
    "\n",
    "dataset_train_tokenized = dataset_train.map(\n",
    "    lambda x: {\"token_ids\": map_text_to_indices(x[\"sentence\"])}\n",
    ")\n",
    "print(dataset_train_tokenized)\n",
    "print(dataset_train_tokenized[0])\n",
    "print(dataset_train_tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_tokenized = dataset_train_tokenized.with_format(columns=[\"token_ids\", \"label\"])\n",
    "\n",
    "def pad_inputs(batch, keys_to_pad=[\"token_ids\"]):\n",
    "    # Get maximum length in batch\n",
    "    max_len = max([len(sample[\"token_ids\"]) for sample in batch])\n",
    "    # Pad inputs to the maximum length\n",
    "    padded_batch = {key: torch.tensor([sample[key] + [0] * (max_len - len(sample[key])) for sample in batch]) for key in keys_to_pad}\n",
    "    # Add remaining keys to the batch\n",
    "    for key in batch[0].keys():\n",
    "        if key not in keys_to_pad:\n",
    "            padded_batch[key] = [sample[key] for sample in batch]\n",
    "    return padded_batch\n",
    "\n",
    "dataloader = DataLoader(dataset_train_tokenized, batch_size=4, collate_fn=pad_inputs)\n",
    "\n",
    "for batch in dataloader:\n",
    "    token_ids = batch[\"token_ids\"]\n",
    "    labels = batch[\"label\"]\n",
    "    print(token_ids)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
