{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use some Huggingface libraries for downloading and processing the dataset, and therefore need to install them first.\\\n",
    "The first is the datasets library which allows you to download and manipulate datasets.\\\n",
    "See [their guide](https://huggingface.co/docs/datasets/installation) for an introduction to the datasets library if you want to know more.\\\n",
    "The other library helps us in downloading data from the Huggingface Hub.\n",
    "\n",
    "We can simple install them using pip:\n",
    "> Note that `%pip` is a magic command for use in Jupyter Notebooks as discussed in the tutorial on setting up Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis (SA) determines the emotional tone of a text sequence (e.g., a sentence) and classifies it into predefined categories.\\\n",
    "Therefore, SA is a text classification task which assigns a single class to the whole input.\n",
    "\n",
    "In this part, we will use the SST2 dataset, which stands for Stanford Sentiment Treebank.\\\n",
    "This is a binary task where inputs are labeled as `positive` (`1`) or `negative` (`0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "There are several possibiliites for downloading datasets.\\\n",
    "You can read them from files, or directly use a library with an online repository of datasets, such as [Huggingface Datasets](https://huggingface.co/docs/datasets/index).\n",
    "\n",
    "> A note on the Huggingface Datasets library: You can decide where downloaded files are cached, see https://huggingface.co/docs/datasets/v3.2.0/en/cache#cache-directory.\\\n",
    "> The documentation is as always a good source of information!\n",
    "\n",
    "We will use this library in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2 = load_dataset(\"stanfordnlp/sst2\")\n",
    "# Print the dataset object to see the dataset's structure\n",
    "print(sst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about using this library, there is a very useful [tutorial](https://huggingface.co/docs/datasets/v3.2.0/en/tutorial) available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are three splits, and how many samples are contained in each split.\\\n",
    "Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = sst2['train']\n",
    "# Print the first sample in the training dataset\n",
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each sample includes an index, the input sentence and a label.\\\n",
    "This sample was labeled `negative` (`0`; `1` stand for `positive` in this dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In order to turn words into features, we can use pre-trained embeddings.\\\n",
    "These have been trained to carry the semantics of the words.\\\n",
    "See https://nlp.stanford.edu/projects/glove/ for more information.\n",
    "\n",
    "GloVe embeddings exist trained on different data (varying in the number of tokens and vocabulary size), and with different embedding dimension.\\\n",
    "In this tutorial, we use the smallest vocabulary size with 300-dimensional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe embeddings\n",
    "glove = hf_hub_download(\"stanfordnlp/glove\", \"glove.6B.zip\")\n",
    "\n",
    "# Unpack the downloaded file\n",
    "word_to_indices = dict()\n",
    "embeddings = []\n",
    "# There are multiple files with different dimensionality of the features in the zip archive: 50d, 100d, 200d, 300d\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        features = torch.tensor([float(value) for value in values[1:]])\n",
    "        word_to_indices[word] = idx\n",
    "        embeddings.append(features)\n",
    "\n",
    "# Last token in the vocabulary is '<unk>' which is used for out-of-vocabulary words\n",
    "# We also add a '<pad>' token to the vocabulary for padding sequences\n",
    "word_to_indices[\"<pad>\"] = len(word_to_indices)\n",
    "embeddings.append(torch.zeros(embeddings[0].shape))\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Number of words in the GloVe embeddings: {len(embeddings)}\")\n",
    "print(f\"Embedding shape: {embeddings.size(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Starting from our input sentences, we need to 1) tokenize the sentences to get smaller units (words or tokens) and 2) convert these tokens into vector representations using the pre-trained embeddings.\n",
    "\n",
    "The datasets library provides support for processing datasets, for more information see https://huggingface.co/docs/datasets/process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def map_token_to_index(token):\n",
    "    # Return the index of the token or the index of the '<unk>' token if the token is not in the vocabulary\n",
    "    return word_to_indices.get(token, word_to_indices[\"<unk>\"])\n",
    "\n",
    "\n",
    "def map_text_to_indices(text: str):\n",
    "    return [map_token_to_index(token) for token in tokenize(text)]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    return dataset.map(lambda x: {\"token_ids\": map_text_to_indices(x[\"sentence\"])})\n",
    "\n",
    "dataset_train_tokenized = prepare_dataset(dataset_train)\n",
    "\n",
    "# Print the first sample in the tokenized training dataset\n",
    "print(dataset_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a dataloader that takes care of batching our data.\\\n",
    "You have seen this before, but this time we need also take care of padding, since the length of the sentences varies in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the columns that we want to keep in the dataset\n",
    "dataset_train_tokenized = dataset_train_tokenized.with_format(\n",
    "    columns=[\"token_ids\", \"label\"]\n",
    ")\n",
    "\n",
    "def pad_inputs(batch, keys_to_pad=[\"token_ids\"], padding_value=-1):\n",
    "    # Pad inputs to the maximum length\n",
    "    padded_batch = {}\n",
    "    for key in keys_to_pad:\n",
    "        # Get maximum length in batch\n",
    "        max_len = max([len(sample[key]) for sample in batch])\n",
    "        # Pad all samples to the maximum length\n",
    "        padded_batch[key] = torch.tensor(\n",
    "            [\n",
    "                sample[key] + [padding_value] * (max_len - len(sample[key]))\n",
    "                for sample in batch\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Add remaining keys to the batch\n",
    "    for key in batch[0].keys():\n",
    "        if key not in keys_to_pad:\n",
    "            padded_batch[key] = torch.tensor([sample[key] for sample in batch])\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train_tokenized,\n",
    "    batch_size=4,\n",
    "    collate_fn=partial(pad_inputs, padding_value=word_to_indices[\"<pad>\"]),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    token_ids = batch[\"token_ids\"]\n",
    "    labels = batch[\"label\"]\n",
    "    print(token_ids)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now having processed the data, it is time for creating a neural network for training.\\\n",
    "We will use a simple network here with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(torch.nn.Module):\n",
    "    def __init__(self, embeddings, hidden_size=128, padding_index=-1):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "            embeddings, freeze=True, padding_idx=padding_index\n",
    "        )\n",
    "        self.layer1 = torch.nn.Linear(embeddings.shape[1], hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of our model, we compute the accuracy and optionally also the loss on the provided dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    return torch.sum(torch.argmax(predictions, dim=1) == labels).item() / len(labels)\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, loss_fn=None):\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=4,\n",
    "        collate_fn=partial(pad_inputs, padding_value=word_to_indices[\"<pad>\"]),\n",
    "    )\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    # We don't need to compute gradients for the evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            token_ids = batch[\"token_ids\"]\n",
    "            labels = batch[\"label\"]\n",
    "            predictions = model(token_ids)\n",
    "            if loss_fn:\n",
    "                loss = loss_fn(predictions, labels)\n",
    "                losses.append(loss.item())\n",
    "            accuracies.append(compute_accuracy(predictions, labels))\n",
    "    return sum(accuracies) / len(accuracies), (sum(losses) / len(losses)) if loss_fn else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to evaluate our model on a separate dataset, we will also have to process this similarly to our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = sst2[\"validation\"]\n",
    "dataset_val_tokenized = prepare_dataset(dataset_val)\n",
    "dataset_val_tokenized = dataset_val_tokenized.with_format(\n",
    "    columns=[\"token_ids\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(embeddings, padding_index=word_to_indices[\"<pad>\"])\n",
    "accuracy = evaluate_model(model, dataset_val_tokenized)\n",
    "print(f\"Accuracy on the validation dataset: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With randomly initialized weights, we expect to end up at ~50% Accuracy (in average)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual training, we also need a loss function and an optimizer.\\\n",
    "The rest is just a simple training loop as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train, losses_val = [], []\n",
    "accuracies_train, accuracies_val = [], []\n",
    "\n",
    "# Compute loss and accuracy on the training set\n",
    "accuracy, loss = evaluate_model(model, dataset_train_tokenized, loss_fn)\n",
    "losses_train.append(loss)\n",
    "accuracies_train.append(accuracy)\n",
    "\n",
    "# Compute loss and accuracy on the validation set\n",
    "accuracy, loss = evaluate_model(model, dataset_val_tokenized, loss_fn)\n",
    "losses_val.append(loss)\n",
    "accuracies_val.append(accuracy)\n",
    "\n",
    "# Do one epoch of training\n",
    "pbar = trange(20)\n",
    "for epoch in pbar:\n",
    "    for batch in dataloader_train:\n",
    "        token_ids = batch[\"token_ids\"]\n",
    "        labels = batch[\"label\"]\n",
    "        predictions = model(token_ids)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the loss and accuracy on the training set\n",
    "    acc_train, loss_train = evaluate_model(model, dataset_train_tokenized, loss_fn)\n",
    "    accuracies_train.append(acc_train)\n",
    "    losses_train.append(loss_train)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    acc_val, loss_val = evaluate_model(model, dataset_val_tokenized, loss_fn)\n",
    "    accuracies_val.append(acc_val)\n",
    "    losses_val.append(loss_val)\n",
    "\n",
    "    pbar.set_postfix_str(f\"Train loss: {losses_train[-1]} - Validation acc: {accuracies_val[-1]}\")\n",
    "\n",
    "# Visualize the loss and accuracy\n",
    "plt.plot(losses_train, color=\"orange\", linestyle=\"-\", label=\"Train loss\")\n",
    "plt.plot(losses_val, color=\"orange\", linestyle=\"--\", label=\"Validation loss\")\n",
    "plt.plot(accuracies_train, color=\"steelblue\", linestyle=\"-\", label=\"Train accuracy\")\n",
    "plt.plot(accuracies_val, color=\"steelblue\", linestyle=\"--\", label=\"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
