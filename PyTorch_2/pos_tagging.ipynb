{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch POS Tagging\n",
    "\n",
    "## Requirements\n",
    "- PyTorch\n",
    "- huggingface_hub\n",
    "- datasets\n",
    "- tqdm\n",
    "- spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pip, check conda online!\n",
    "%pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the POS tagging dataset from the Hugging Face hub and prepares it for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"batterydata/pos_tagging\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have a training and a test dataset, so we use some training samples for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset[\"train\"].train_test_split(test_size=0.1, shuffle=True)\n",
    "dataset[\"validation\"] = dataset_split[\"test\"]\n",
    "dataset[\"train\"] = dataset_split[\"train\"]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unpack GloVe 300d embeddings from a zip file, build a word-to-index dictionary, and store each word's embedding vector in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unpack the GloVe embeddings\n",
    "glove = hf_hub_download(\"stanfordnlp/glove\", \"glove.6B.zip\")\n",
    "\n",
    "# There are multiple files with different dimensionality of the features in the zip archive: 50d, 100d, 200d, 300d\n",
    "filename = \"glove.6B.300d.txt\"\n",
    "\n",
    "word_to_index = dict()\n",
    "embeddings = []\n",
    "\n",
    "with zipfile.ZipFile(glove, \"r\") as f:\n",
    "    for idx, line in enumerate(f.open(filename)):\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\")\n",
    "        features = torch.tensor([float(value) for value in values[1:]])\n",
    "        word_to_index[word] = idx\n",
    "        embeddings.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add padding and unknown tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last token in the vocabulary is '<unk>' which is used for out-of-vocabulary words\n",
    "# We also add a '<pad>' token to the vocabulary for padding sequences\n",
    "word_to_index[\"<pad>\"] = len(word_to_index)\n",
    "padding_token_id = word_to_index[\"<pad>\"]\n",
    "unk_token_id = word_to_index[\"<unk>\"]\n",
    "\n",
    "embeddings.append(torch.zeros(embeddings[0].shape))\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.size(1)}\")\n",
    "print(f\"Padding token id: {padding_token_id}\")\n",
    "print(f\"Unknown token id: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionaries to map labels to indices and vice versa, and print the number of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_unique = list(\n",
    "    set([label for sample in dataset[\"train\"] for label in sample[\"labels\"]])\n",
    ")\n",
    "print(labels_unique)\n",
    "print(f\"Number of classes: {len(labels_unique)}\")\n",
    "ctoi = {label: idx for idx, label in enumerate(labels_unique)}\n",
    "itoc = {idx: label for label, idx in ctoi.items()}\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map tokens and labels to indices, and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_list_using_dict(mapping, keys: list, default=None):\n",
    "    return [mapping.get(key, default) for key in keys]\n",
    "\n",
    "\n",
    "def map_tokens_to_indices(tokens: list[str]):\n",
    "    # Return the index of each token or the index of the '<unk>' token if a token is not in the vocabulary\n",
    "    return map_list_using_dict(\n",
    "        word_to_index, [token.lower() for token in tokens], unk_token_id\n",
    "    )\n",
    "\n",
    "\n",
    "def map_labels_to_indices(labels: list):\n",
    "    # TODO: Implement the mapping of the labels to indices\n",
    "    return NotImplementedError\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    # return map(lambda x: {\"token_ids\": map_text_to_indices(x[\"words\"])}, dataset)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\n",
    "            \"token_ids\": map_tokens_to_indices(x[\"words\"]),\n",
    "            \"label_ids\": map_labels_to_indices(x[\"labels\"]),\n",
    "        },\n",
    "        num_proc=1,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = prepare_dataset(dataset)\n",
    "dataset_train_tokenized = dataset[\"train\"]\n",
    "dataset_validation_tokenized = dataset[\"validation\"]\n",
    "\n",
    "# Print the first sample in the tokenized training dataset\n",
    "print(dataset_train_tokenized[0].keys())\n",
    "print(dataset_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again pad inputs to the maximum sequence length in the batch.\\\n",
    "But this time, we also have to pad the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_inputs(batch, keys_to_pad=[\"token_ids\", \"label_ids\"], padding_value=-1):\n",
    "    # Pad keys_to_pad to the maximum length in batch\n",
    "    padded_batch = {}\n",
    "    for key in keys_to_pad:\n",
    "        # Get maximum length in batch\n",
    "        max_len = max([len(sample[key]) for sample in batch])\n",
    "        # Pad all samples to the maximum length\n",
    "        padded_batch[key] = torch.tensor(\n",
    "            [\n",
    "                sample[key] + [padding_value] * (max_len - len(sample[key]))\n",
    "                for sample in batch\n",
    "            ]\n",
    "        )\n",
    "    # Add remaining keys to the batch\n",
    "    for key in batch[0].keys():\n",
    "        if key not in keys_to_pad:\n",
    "            padded_batch[key] = torch.tensor([sample[key] for sample in batch])\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, batch_size=32, shuffle=False):\n",
    "    # Create a DataLoader for the dataset\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(pad_inputs, padding_value=padding_token_id),\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a DataLoader for the training dataset with the selected columns\n",
    "dataloader_train = get_dataloader(\n",
    "    dataset_train_tokenized.with_format(columns=[\"token_ids\", \"label_ids\"]),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "dataloader_validation = get_dataloader(\n",
    "    dataset_validation_tokenized.with_format(columns=[\"token_ids\", \"label_ids\"]),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "for batch in dataloader_train:\n",
    "    token_ids = batch[\"token_ids\"]\n",
    "    labels = batch[\"label_ids\"]\n",
    "    print(token_ids)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs\n",
    "\n",
    "So far, we have not paid attention to which device the PyTorch operations are running on.\\\n",
    "By default, they run on the CPU, however, a GPU is usually much faster when performing tensor operations.\\\n",
    "For this, you will need to have a supported GPU available on the device where you execute this code.\\\n",
    "Our servers at the IMS provide GPUs (strauss, nandu, kiwi).\\\n",
    "You can either remotely connect your editor and run the code there, or connect to a remote Python Kernel.\n",
    "\n",
    "Once there is a supported GPU available on your machine that runs the code, you can copy tensors and even models using the method `.to(device)` to `device`.\\\n",
    "`device` can be specified using `torch.device`:\n",
    "```python\n",
    "# 'cuda' for GPU (optionally specify device id, e.g., 'cuda:0' for the first GPU) and 'cpu' for CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "\n",
    "> Note that not specifying a device id might use all GPUs! Therefore, always set the device id or restrict the available GPUs, e.g., using the environment variable `CUDA_VISIBLE_DEVICES`.\n",
    "> You can set this variable first using `export CUDA_VISIBLE_DEVICES=3` so that any executed command afterward will use the GPU with id 3 (the 4th GPU) or directly set it for your command using `CUDA_VISIBLE_DEVICES=3 command`.\n",
    "\n",
    "You may also allocate tensors on a specific device during intialization:\n",
    "```python\n",
    "a = torch.tensor(..., device=device)\n",
    "```\n",
    "This works for all the tensor creation operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our neural network consists of one fully connected linear layer\n",
    "\n",
    "The softmax is part of the loss function in PyTorch, so you can omit this in the forward function.\n",
    "\n",
    "The embedding layer\n",
    "- maps from indices to vectors\n",
    "- is not trained (freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePOSTagger(torch.nn.Module):\n",
    "    # this resembles a really simple neural network: an embedding layer followed by a fully\n",
    "    # connected linear layer such that predictions are computed for each token in the sequence\n",
    "    # and batch independently\n",
    "    def __init__(self, embedding_vectors, num_classes, hidden_dim):\n",
    "        super().__init__()\n",
    "        # PyTorch's embedding layer maps from indices to embeddings, freeze will tell PyTorch to\n",
    "        # not train this layer, i.e. not modifying any weight\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "            embedding_vectors, freeze=True\n",
    "        )\n",
    "        # a fully connected linear layer mapping the embedded vector to a vector of fixed size\n",
    "        # (num_classes in this case)\n",
    "        self.hidden_layer = torch.nn.Linear(embedding_vectors.size(1), hidden_dim)\n",
    "        self.output_layer = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # simple forwarding through our model\n",
    "        # PyTorch takes care of keeping track of the operations for the backward pass\n",
    "        emmedded_inputs = self.embedding(inputs)\n",
    "        z_1 = self.hidden_layer(emmedded_inputs)\n",
    "        a_1 = torch.nn.functional.leaky_relu(z_1, negative_slope=0.2)\n",
    "        z_2 = self.output_layer(a_1)\n",
    "        return z_2  # softmax is applied in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model, loss and optimizer\n",
    "- Cross Entropy is Softmax + Negative Log Likelihood\n",
    "- As optimizer we use Adam (adapts the learning rate per weight)\n",
    "\n",
    "(run this only once as Jupyter keeps the model (including the weights) and the optimizer in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and optimizer and move model to device\n",
    "model = SimplePOSTagger(embedding_vectors=embeddings, num_classes=len(ctoi), hidden_dim=128).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index=padding_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "metric_dict = {'loss': '------', 'accuracy': '------'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function comparing prediction with gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_iter, model):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # extract input and labels\n",
    "\n",
    "        # move data to device since our model is on the device\n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # predict only\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_ids)\n",
    "        outputs_classes = outputs.argmax(dim=2)\n",
    "\n",
    "        # compute amount of correct predictions\n",
    "        # sequence lengths within the batch might be different, so we need to take care of that\n",
    "        inputs_lengths = (token_ids != 400001).sum(dim=1)\n",
    "\n",
    "        total_count += inputs_lengths.sum()\n",
    "        # iterate over each sample of the batch\n",
    "        batch_size = outputs_classes.size(0)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(inputs_lengths[i]):\n",
    "                correct_count += int(outputs_classes[i][j] == labels[i][j])\n",
    "    return correct_count / total_count.float().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(dataloader_validation, model)\n",
    "print(f\"Accuracy on the validation dataset: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual training loop\n",
    "\n",
    "- runs several epochs\n",
    "- in each epoch\n",
    " - forward the batch\n",
    " - computes the loss for the output of the whole batch\n",
    " - reduces (e.g. average, sum) the loss\n",
    " - computes derivatives of weights by backpropagation\n",
    " - optimizer updates weights\n",
    " - evaluate on validation/development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# a nice progress bar to make the waiting time much better\n",
    "pbar = tqdm(total=NUM_EPOCHS*len(dataloader_train), postfix=metric_dict)\n",
    "\n",
    "# evaluate on validation set first\n",
    "metric_dict.update({'accuracy': f'{100*evaluate(dataloader_validation, model):6.2f}%'})\n",
    "pbar.set_postfix(metric_dict)\n",
    "\n",
    "# run for NUM_EPOCHS epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # run for every data (in batches) of our iterator\n",
    "    \n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        # extract input and labels and move data to device since our model is on the device\n",
    "        token_ids = batch[\"token_ids\"].to(device=DEVICE)\n",
    "        labels = batch[\"label_ids\"].to(device=DEVICE)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(token_ids)\n",
    "        \n",
    "        # 2D loss function expects input as (batch, prediction, sequence) and target as (batch, sequence) containing the class index\n",
    "        loss = criterion(outputs.permute(0,2,1), labels)\n",
    "        # otherwise use view function to get rid of sequence dimension by effectively concatenating all sequence items\n",
    "        # loss = criterion(outputs.view(-1, len(classes)), labels.view(-1))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        pbar.update()\n",
    "        metric_dict.update({'loss': f'{loss.item():6.3f}'})\n",
    "        pbar.set_postfix(metric_dict)\n",
    "        \n",
    "    # evaluate on validation set after each epoch\n",
    "    metric_dict.update({'accuracy': f'{100*evaluate(dataloader_validation, model):6.2f}%'})\n",
    "    pbar.set_postfix(metric_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly predict sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_indices_to_labels(indices: list):\n",
    "    return map_list_using_dict(itoc, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a sample from the validation dataset\n",
    "sample = random.choice(dataset_validation_tokenized)\n",
    "print(sample)\n",
    "# build input vector and add batch dimension\n",
    "sample_tensor = torch.tensor(sample[\"token_ids\"]).unsqueeze(dim=0).to(DEVICE)\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    # get rid of batch dimension (is set to 1)\n",
    "    outputs = model(sample_tensor).squeeze(dim=0)\n",
    "\n",
    "predictions = [itoc[output.argmax(dim=0).item()] for output in outputs]\n",
    "print(\"Input:\", ' '.join(sample[\"words\"]))\n",
    "print(f\"Prediction:   {predictions}\")\n",
    "print(f\"Ground truth: {sample['labels']}\")\n",
    "accuracy = sum([1 for pred, gt in zip(predictions, sample[\"labels\"]) if pred == gt]) / len(sample[\"labels\"])\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not have to tokenize our data so far since tokens were given.\\\n",
    "For tokenizing text, you can again use the tokenization from the sentiment analysis task, but it has some trouble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_simple(text: str):\n",
    "    return text.lower().split()\n",
    "\n",
    "print(tokenize_simple(\"This is a simple text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation is not properly split, but for POS tagging to work correctly, we need punctuation is separate tokens too.\\\n",
    "We can extract words and punctuation using a regular expression (regex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_regex(text: str):\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "\n",
    "print(tokenize_regex(\"This is a simple text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also packages like spacy that help you with tokenization.\\\n",
    "We have to install it first and then download some files for the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy using pip\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download resources for english\n",
    "# `run` has to be replaced by `python` if run in a shell\n",
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return [token.text for token in nlp(text.lower())]\n",
    "\n",
    "print(tokenize(\"This is a simple text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We render a nice text box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "sentence_widget = widgets.Text(\n",
    "    value=\"This movie is terrible\",\n",
    "    placeholder=\"Type something\",\n",
    "    description=\"Sentence:\",\n",
    "    disabled=False,\n",
    ")\n",
    "display(sentence_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Prepare the input, and feed it through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentence_widget.value\n",
    "\n",
    "# convert text to token ids\n",
    "\n",
    "# build input vector and add batch dimension\n",
    "\n",
    "\n",
    "# forward / predict\n",
    "with torch.no_grad():\n",
    "    \n",
    "\n",
    "# print prediction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
