{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradients\n",
    "## Calculating gradients\n",
    "\n",
    "In PyTorch, it's as easy as calling `backward()` on a tensor.\n",
    "Only requirement: the tensor has to be enabled for gradient calculation.\n",
    "If you look at our previous tensors e.g., they won't have the `requires_grad` attribute set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easy to fix: we can just tell PyTorch that a tensor should record gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "z = x * 3 + y + 2\n",
    "print(\"z = x * 3 + y + 2 =\", z)\n",
    "u = z * z\n",
    "print(\"u = z * z =\", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, PyTorch will record all differentiable operations (see `grad_fn`) for the contributing tensors (that were not done in-place) and we can calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.retain_grad() # keeps gradient info for z instead of only for the leave nodes x,y\n",
    "# backward pass from u\n",
    "u.backward()\n",
    "print(\"du/dz =\", z.grad) # du/dz\n",
    "print(\"du/dx =\", x.grad) # du/dx\n",
    "print(\"du/dy =\", y.grad) # du/dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Don't try to backward multiple times on the same graph instance, it will throw an error (you probably don't want to do this anyway)! \n",
    "If you really have to do this, pass the parameter `retain_graph=True` as an argument to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We usually only need to call backward on the output of our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also assign elements in a vector.\\\n",
    "There is an important difference though:\\\n",
    "**NEVER modify individual tensor entries in-place if you want to calculate a gradient later** (those operations are not recorded for the computational graph)!\\\n",
    "Choose to use PyTorch operations instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = torch.zeros(2,2, requires_grad=True)\n",
    "bad[0,0] = 1.0 # in-place modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = torch.zeros(2,2, requires_grad=True)\n",
    "good = good + torch.eye(2) # re-assignment: new tensor is created, but you loose access to original tensor `good`\n",
    "good.retain_grad() # only to be able to access `grad` attribute later\n",
    "print(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily get a different view of the tensor, e.g. flatten it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_loss = good.view(-1).sum(dim=0) # flatten tensor first, then sum all values\n",
    "print(good_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_loss.backward() # success -> gradients computed\n",
    "print(good.grad) # the computed gradient for tensor `good`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAUTION: Gradients are accumulated hence you have to zero them after you've used them (e.g. after optimizing your weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_loss.backward() # compute gradients again\n",
    "print(good.grad)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "44d183e2ed7f9e41e8dce83aa5d5fcdc7ada6d3c3409baab8ecd401fc159e44c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
