{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A full training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at a 2-class classification problem with a generated toy-dataset.\n",
    "The data will be stored inside a PyTorch `Dataset` subclass, which will provide you with some utilities like automatic shuffling and batching for the training loop, if paired with `DataLoader`.\n",
    "\n",
    "The 2 classes are two normal distributions with different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib # install using pip\n",
    "#%conda install -y matplotlib # or install using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class TwoClassDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # create synthetic dataset\n",
    "        self.features_a = torch.normal(mean=1., std=1.0, size=(100,2))\n",
    "        self.labels_a = torch.zeros(100, dtype=torch.long)\n",
    "        self.features_b = torch.normal(mean=-1.0, std=1.0, size=(100,2))\n",
    "        self.labels_b = torch.ones(100, dtype=torch.long)\n",
    "\n",
    "        self.data = torch.cat((self.features_a, self.features_b), dim=0) # X: 200 x 2 matrix : [x1,x2]\n",
    "        self.labels = torch.cat((self.labels_a, self.labels_b), dim=0) # Y: binary vector of length 200: 0: class A, 1: class B\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.size(dim=0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "    \n",
    "train_data = TwoClassDataset()\n",
    "\n",
    "# plot both classes\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_data.features_a[:,0], train_data.features_a[:,1], label=\"class 1\", c=\"darkorange\")\n",
    "ax.scatter(train_data.features_b[:,0], train_data.features_b[:,1], label=\"class 2\", c=\"dodgerblue\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first sample input, label:\", train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader can shuffle your dataset for each epoch (which you should definitely do to keep mini-batches representative) and also automatically groups your single samples together into matrices for a mini-batch (the first dimension will always be the batch dimension, e.g. if your data point has size $3$, then a batch of size 25 would have size $25 \\times 3$.\n",
    "\n",
    "Take a look at the basics about dataloaders and datasets: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "How can we create a DataLoader for our `TwoClassDataset` with a batch size of 5 and shuffling enabled?\n",
    "- Print a sample from the dataset\n",
    "- Print the training data samples and labels separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataLoader\n",
    "\n",
    "# print data and labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're creating a new clasifier for our problem, the input size is 2 and the output size is 2 because we want to do multi-class classification (for 2 classes, a sigmoid and a single output would also work).\n",
    "The activation function is a ReLU (rectified linear unit). It is piecewise linear: $relu(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define the layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "#%conda install -y tqdm # or install using conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training loop, we need instances of\n",
    "* the training data (in real applications, also development / test data)\n",
    "* a loss criterion (e.g. MSE for regression tasks, Cross Entropy for classification)\n",
    "* an optimizer\n",
    "\n",
    "We will iterate many times over the full dataset (each iteration is called an *epoch*).\n",
    "Each epoch is broken up further into mini-batches that can be adapted to fit e.g. your computer's memory constraints.\n",
    "\n",
    "For each mini-batch, we need to:\n",
    "* reset the gradients (otherwise, they will accumulate)\n",
    "* forward the input data to create predictions\n",
    "* calculate the loss between the predictions and the gold labels\n",
    "* calculate the gradient of the loss w.r.t. the network weights (backward pass)\n",
    "* update the network weights using the gradients and the optimizer\n",
    "\n",
    "Please implement the following steps to complete the training loop:\n",
    "\n",
    "1. **Reset gradients to 0**\n",
    "   - Task: Reset the gradients to zero using the optimizer.\n",
    "   - Variable: `optim`\n",
    "\n",
    "2. **Access data and labels from batch**\n",
    "   - Task: Access the input data and labels from the current mini-batch.\n",
    "   - Variables: `inputs`, `gold_labels`\n",
    "\n",
    "3. **Perform the forward pass**\n",
    "   - Task: Perform the forward pass to get the predictions from the model.\n",
    "   - Variables: `predictions`, `model`\n",
    "\n",
    "4. **Calculate the loss and perform the backward pass**\n",
    "   - Task: Calculate the loss and perform the backward pass to compute the gradients.\n",
    "   - Variables: `loss`, `loss_func`\n",
    "\n",
    "5. **Update the network weights**\n",
    "   - Task: Update the network weights using the optimizer.\n",
    "   - Variable: `optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# instantiate model, loss criterion and optimizer\n",
    "classy = Classifier()\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(params=classy.parameters(), lr=0.01)\n",
    "\n",
    "# store epoch metrics\n",
    "epoch_accs = []\n",
    "epoch_losses = []\n",
    "dataloader = DataLoader(dataset=train_data, batch_size=25, shuffle=True)\n",
    "\n",
    "# epoch loop\n",
    "for epoch in tqdm(range(50)):\n",
    "    epoch_acc = []\n",
    "    epoch_loss = []\n",
    "    \n",
    "    # mini-batch loop for one epoch\n",
    "    for batch in dataloader:\n",
    "        # reset gradients to 0\n",
    "        \n",
    "        # access data and labels from batch\n",
    "\n",
    "        # forward pass\n",
    "       \n",
    "        # loss and backward pass\n",
    "        \n",
    "        # update network weights\n",
    "        \n",
    "        # check accuracy (get predicted class for each sample, compare to gold label)\n",
    "        category_probs = torch.softmax(predictions, dim=1) # sums up to 1 for each sample\n",
    "        category_labels = torch.argmax(category_probs, dim=1) # extract most likely label\n",
    "        batch_acc = (category_labels == gold_labels).float().sum(dim=0)/25.0 # avg accuracy for batch\n",
    "        epoch_acc.append(batch_acc.item())\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    # average all metrics across one epoch\n",
    "    epoch_losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    epoch_accs.append(sum(epoch_acc)/len(epoch_acc))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize the predictions for an unseen test dataset drawn from the same distribution as the training data. Red points mark misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "test_data = TwoClassDataset()\n",
    "\n",
    "predicted_class_a = []\n",
    "predicted_class_b = []\n",
    "missclassified_class_a = []\n",
    "missclassified_class_b = []\n",
    "for i in range(len(test_data)):\n",
    "    x, label = test_data[i]\n",
    "    pred = classy(x)\n",
    "    probs = torch.softmax(pred, dim=0) # note that the softmax is mathematically not needed since it is a monotonic function\n",
    "    class_pred = torch.argmax(probs, dim=0)\n",
    "  \n",
    "    if label != class_pred:\n",
    "        if label == 0:\n",
    "            missclassified_class_a.append(x.unsqueeze(dim=0))\n",
    "        else:\n",
    "            missclassified_class_b.append(x.unsqueeze(dim=0))\n",
    "    else:\n",
    "        if class_pred == 0:\n",
    "            predicted_class_a.append(x.unsqueeze(dim=0))\n",
    "        else:\n",
    "            predicted_class_b.append(x.unsqueeze(dim=0))\n",
    "predicted_class_a = torch.cat(predicted_class_a, dim=0) # concatenates tensor along specified, existing dimension, does not create new dimension\n",
    "predicted_class_b = torch.cat(predicted_class_b, dim=0) # `stack` is an alternative which creates a new dimension (i.e. we wouldn't have `unsqueeze`d the vectors)\n",
    "missclassified_class_a = torch.cat(missclassified_class_a, dim=0)\n",
    "missclassified_class_b = torch.cat(missclassified_class_b, dim=0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(predicted_class_a[:,0], predicted_class_a[:,1], label=\"class 1\", c=\"darkorange\")\n",
    "ax.scatter(predicted_class_b[:,0], predicted_class_b[:,1], label=\"class 2\", c=\"dodgerblue\")\n",
    "ax.scatter(missclassified_class_a[:,0], missclassified_class_a[:,1], c=\"darkorange\", edgecolors=\"r\")\n",
    "ax.scatter(missclassified_class_b[:,0], missclassified_class_b[:,1], c=\"dodgerblue\", edgecolors=\"r\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
