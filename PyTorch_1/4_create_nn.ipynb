{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description:\n",
    "* A neural network consists of neurons\n",
    "* Most common function a neuron calculates is linear: $z = \\mathbf{w}*\\mathbf{x} + b$, where $\\mathbf{x} \\in \\mathbb{R}^{nÂ \\times 1}$ is the inputs to a neuron, $\\mathbf{w} \\in \\mathbb{R}^{1 \\times n}$ are trainable weights and $b$ is a scalar trainable bias term\n",
    "* To give a neuron more representational power (i.e. learning non-linear functions), each neuron is followed by a non-linear *activation function f*, e.g. $y = f(z) = sigmoid(z)$. \n",
    "* Neurons are arranged in layers (see image below) that can be stacked\n",
    "* The math of a single neuron then generalizes to a layer of neurons via $\\mathbf{z} = W * \\mathbf{x} + \\mathbf{b}$ with the weights for all neurons collected in a matrix $W \\in \\mathbb{R}^{m \\times n}$, the bias of all neurons as a vector $b \\in \\mathbb{R}^m$ and the input to the layer as vector $\\mathbf{x} \\in \\mathbb{R}^n$. The output of a layer will then be a vector $\\mathbf{z} \\in \\mathbb{R}^m$ - each column represents the weights w.r.t one input connection, each row one neuron, yielding the transformation $\\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{m \\times 1}$. \n",
    "* The activation function is then applied element-wise to $\\mathbf{z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png\" width=500/>\\\n",
    "source: https://miro.medium.com/max/1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an example input (input layer), which is just a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4) # 4-dim vector\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a linear layer can be instantiated by giving the input size (n) and output size (m) of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Linear(4, 5, bias=True) # 4 inputs, 5 outputs -> 5 neurons\n",
    "# W\n",
    "print(\"W =\", layer.weight) # 5x4 tensor connecting all inputs to all outputs\n",
    "# b\n",
    "print(\"b =\", layer.bias) # tensor of size 5, one bias for each output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forward the input x through our layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = layer(x)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now only missing an activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.sigmoid(z1)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and that's our first layer! \n",
    "We can create the other layers in the same fashion.\n",
    "But to organize everything together into one model, it is useful to subclass from `torch.nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create layers: input 4, output 5\n",
    "        self.layer1 = torch.nn.Linear(4,5)\n",
    "        # input 5, output 7\n",
    "        self.layer2 = torch.nn.Linear(5,7)\n",
    "        # input 7, output 3\n",
    "        self.layer3 = torch.nn.Linear(7,3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass (with sigmoid activations) will be called on self()\n",
    "        y1 = torch.sigmoid(self.layer1(x)) # layer 1 + activation\n",
    "        y2 = torch.sigmoid(self.layer2(y1)) # layer 2 + activation\n",
    "        y3 = self.layer3(y2) # layer 3: no activation function, with sigmoid we could only get outputs between [0,1]\n",
    "       \n",
    "        return y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell PyTorch where tensors are stored and therefore where the computations run.\\\n",
    "A GPU is usually much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().to(device) # instantiate model and move to device\n",
    "x = x.to(device) # note that tensors of an operation need to be on the same device\n",
    "y = model.forward(x) # directly call the forward function, don't use this!\n",
    "y = model(x) # same as model.forward(x) but with extended pre-processing\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning works by adjusting the weights w.r.t to an objective function \n",
    "    * calculate the objective function based on calculated output vs. real labels from dataset\n",
    "    * take gradients of objective function w.r.t. network weights and biases\n",
    "    * use a gradient-based optimizer to update the existing weights using the gradients\n",
    "* An example objective function could be quadratic loss:\n",
    "    $L_2(y, y') = (y - \\hat{y})^2$\n",
    "* Extending the loss to multiple samples could e.g. be Mean-Squared-Error loss: $\\frac{1}{M}\\sum_{i=1}^{M} (y_i - \\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss() # PyTorch knows several loss/cost/objective functions\n",
    "y_hat = torch.ones(3, device=device) # our dummy label (3-dim) to compare output to - note that we can also specify the device on tensor creation\n",
    "loss = loss_func(y, y_hat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the backward pass (calculating gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(\"dMSE/dW =\", model.layer1.weight.grad) # print gradients for W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the weights of the model is now a matter of choosing the step size for subtracting the gradients from the respective weights.\n",
    "PyTorch comes with several optimizer functions, e.g. stochastic gradient descent (SGD) or more complex ones, e.g. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W_0 =\", model.layer1.weight)\n",
    "optim = torch.optim.SGD(params=model.parameters(), lr=0.3) # tell the optimizer what weights to update and how much\n",
    "optim.step() # update weights by applying gradients to the weights\n",
    "print(\"W_1 =\", model.layer1.weight) # updated gradients for W1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify that our parameters are \"more optimal\" with respect to our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)\n",
    "loss = loss_func(y, y_hat)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-dl-ws-2024-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
